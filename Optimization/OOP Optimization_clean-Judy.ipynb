{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "import hmmlearn as hmm\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import cvxpy as cp\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "## Additions below\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    #Anything Data Related\n",
    "    def __init__(self, stock_prices, risk_free, universe=None, period='M'):\n",
    "        #TO-DO: Add initialization of market cap\n",
    "        \n",
    "        if not universe:\n",
    "            universe = stock_prices.columns\n",
    "            \n",
    "        if type(universe[0]) == int:\n",
    "            self.stock_prices = stock_prices.iloc[:,universe]\n",
    "\n",
    "        else:\n",
    "            self.stock_prices = stock_prices[universe]\n",
    "        \n",
    "        self.risk_free = risk_free\n",
    "        self.risk_free.index = pd.to_datetime(self.risk_free.index)\n",
    "        self.risk_free = self.risk_free.resample(period).last()\n",
    "        \n",
    "        self.stock_prices.index= pd.to_datetime(self.stock_prices.index)\n",
    "        \n",
    "        self.factor_returns=None\n",
    "        self.stock_returns=self.get_stock_returns()\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def get_stock_returns(self, period='M'):\n",
    "        price = self.stock_prices.resample(period).last()\n",
    "\n",
    "        # Calculate the percent change\n",
    "        ret_data = price.pct_change()[1:]\n",
    "\n",
    "        # Convert from series to dataframe\n",
    "        ret_data = pd.DataFrame(ret_data)\n",
    "\n",
    "        return ret_data\n",
    "\n",
    "    def set_factor_returns(self, factor_type='FF', period='M'):\n",
    "        if factor_type == 'CAPM':\n",
    "            self.factor_returns = self.get_CAPM_returns(period)\n",
    "        \n",
    "        elif factor_type == 'FF':\n",
    "            self.factor_returns = self.get_FF_returns(period)\n",
    "            \n",
    "        elif factor_type == 'Carhart':\n",
    "            self.factor_returns = self.get_Carhart_returns(period)\n",
    "            \n",
    "        elif factor_type == 'PCA':\n",
    "            self.factor_returns = self.get_PCA_returns(period)\n",
    "        \n",
    "        else:\n",
    "            print(\"Invalid input: Please select one of the following factor types: CAPM, FF, Carhart or PCA.\")\n",
    "        \n",
    "        return   \n",
    "    \n",
    "    def get_FF_returns(self, period='M'):\n",
    "        ff_url = \"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_Factors_CSV.zip\"    \n",
    "        # Download the file and save it  \n",
    "        urllib.request.urlretrieve(ff_url,'fama_french.zip')\n",
    "        zip_file = zipfile.ZipFile('fama_french.zip', 'r')    \n",
    "        # Extact the file data\n",
    "        zip_file.extractall()\n",
    "        zip_file.close()    \n",
    "        ff_factors = pd.read_csv('F-F_Research_Data_Factors.csv', skiprows = 3, index_col = 0)   \n",
    "        # Skip null rows\n",
    "        ff_row = ff_factors.isnull().any(1).to_numpy().nonzero()[0][0]\n",
    "\n",
    "        # Read the csv file again with skipped rows\n",
    "        ff_factors = pd.read_csv('F-F_Research_Data_Factors.csv', skiprows = 3, nrows = ff_row, index_col = 0)\n",
    "\n",
    "        # Format the date index\n",
    "        ff_factors.index = pd.to_datetime(ff_factors.index, format= '%Y%m')\n",
    "\n",
    "        # Format dates to end of month\n",
    "        ff_factors.index = ff_factors.index + pd.offsets.MonthEnd()\n",
    "\n",
    "        # Resample the data to correct frequency\n",
    "        ff_factors = ff_factors.resample(period).last()\n",
    "\n",
    "        # Convert from percent to decimal\n",
    "        ff_factors = ff_factors.apply(lambda x: x/ 100)\n",
    "\n",
    "        return ff_factors\n",
    "    \n",
    "    def get_CAPM_returns(self, period='M'):\n",
    "        ff_factors = self.get_FF_returns(period)\n",
    "        \n",
    "        # Remove the unnecessary factors\n",
    "        capm_factors = ff_factors.iloc[:, 0]\n",
    "        \n",
    "        return capm_factors\n",
    "    \n",
    "    def get_Carhart_returns(self, period='M'):\n",
    "        ff_factors = self.get_FF_returns(period)\n",
    "\n",
    "        # Get the momentum factor\n",
    "        momentum_url = \"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Momentum_Factor_CSV.zip\"\n",
    "\n",
    "        # Download the file and save it  \n",
    "        urllib.request.urlretrieve(momentum_url,'momentum.zip')\n",
    "        zip_file = zipfile.ZipFile('momentum.zip', 'r')\n",
    "\n",
    "        # Extact the file data\n",
    "        zip_file.extractall()\n",
    "        zip_file.close()\n",
    "\n",
    "        momentum_factor = pd.read_csv('F-F_Momentum_Factor.csv', skiprows = 13, index_col = 0)\n",
    "\n",
    "        # Skip null rows\n",
    "        row = momentum_factor.isnull().any(1).to_numpy().nonzero()[0][0]\n",
    "\n",
    "        # Read the csv file again with skipped rows\n",
    "        momentum_factor = pd.read_csv('F-F_Momentum_Factor.csv', skiprows = 13, nrows = row, index_col = 0)\n",
    "\n",
    "        # Format the date index\n",
    "        momentum_factor.index = pd.to_datetime(momentum_factor.index, format= '%Y%m')\n",
    "\n",
    "        # Format dates to end of month\n",
    "        momentum_factor.index = momentum_factor.index + pd.offsets.MonthEnd()\n",
    "\n",
    "         # Resample the data to correct frequency\n",
    "        momentum_factor = momentum_factor.resample(period).last()\n",
    "\n",
    "        # Convert from percent to decimal\n",
    "        momentum_factor = momentum_factor.apply(lambda x: x/ 100)\n",
    "\n",
    "        # Combine to create the carhart_factors\n",
    "        carhart_factors = pd.concat([ff_factors, momentum_factor], axis=1).dropna()\n",
    "\n",
    "        return carhart_factors\n",
    "    \n",
    "    def get_PCA_returns(self, period='M'):\n",
    "        exRets = self.get_stock_returns(period=\"D\")\n",
    "        num_stocks = len(exRets.columns)\n",
    "        returns_mat = exRets.to_numpy()\n",
    "        n_dates = returns_mat.shape[0]\n",
    "        n_assets = returns_mat.shape[1]\n",
    "        \n",
    "        demeaned = (returns_mat - returns_mat.mean(axis=0)).transpose()\n",
    "        sigma = 1/(n_dates - 1)*np.matmul(demeaned,demeaned.transpose())\n",
    "        eigval, eigvec = np.linalg.eig(sigma)\n",
    "        \n",
    "        principal_components = np.matmul(eigvec.transpose(),demeaned).transpose()\n",
    "        pca_factors = np.real(principal_components[:,0:100])\n",
    "        \n",
    "        pca_df = pd.DataFrame(pca_factors, index = exRets.index, columns = [str(i) for i in range(num_stocks)])\n",
    "        pca_df = pca_df.resample(period).last()\n",
    "        \n",
    "        return pca_df\n",
    "    \n",
    "    def get_index_from_date(self, date_index_df, date):\n",
    "        return date_index_df.index.get_loc(date)\n",
    "    \n",
    "    def get_lookback_data(self, date_index_df, date, lookback):\n",
    "        end_idx= self.get_index_from_date(date_index_df, date)\n",
    "        return date_index_df.iloc[end_idx-lookback:end_idx]      \n",
    "    \n",
    "    def get_num_stocks(self):\n",
    "        return len(self.stock_returns.columns)\n",
    "\n",
    "    \n",
    "class Portfolio:\n",
    "    #Anything Portfolio related: weights, returns, date-stamped\n",
    "    def __init__(self, data):       \n",
    "        num_stocks=data.get_num_stocks()\n",
    "        self.weights= np.array([[0]*num_stocks + [1]]) # 0 weight on stock\n",
    "        self.returns= np.array([])\n",
    "        self.dates= []\n",
    "        return\n",
    "        \n",
    "    def update_weights(self, new_weights):\n",
    "        \n",
    "        new_weights = np.expand_dims(new_weights, axis=0)\n",
    "        self.weights = np.append(self.weights, new_weights, axis=0)\n",
    "        return\n",
    "     \n",
    "    def update_returns(self, new_returns):\n",
    "        self.returns=np.append(self.returns, new_returns)\n",
    "        return\n",
    "\n",
    "    def update_dates(self, new_dates):\n",
    "       \n",
    "        self.dates.append(new_dates)\n",
    "        return\n",
    "        \n",
    "    def get_Sharpe(self, data):\n",
    "        risk_free = data.risk_free\n",
    "        recent_date = self.dates[-1]\n",
    "        sigma = np.std(self.returns - np.array(risk_free.loc[self.dates]))\n",
    "        sharpe_ratio = ((np.prod(1+self.returns)-1) - np.array(risk_free.loc[recent_date]))/sigma\n",
    "        return sharpe_ratio\n",
    "        \n",
    "    def plot(self):\n",
    "        port_cumu_returns = np.array([x+1 for x in self.returns]).cumprod()\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.plot(self.dates, port_cumu_returns)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Cumulative Return\")\n",
    "        plt.show()\n",
    "    \n",
    "class Costs:\n",
    "    def __init__(self, trans_coeff, holding_coeff):\n",
    "        self.holding_cost = 0\n",
    "        self.trans_cost = 0\n",
    "        self.trans_coeff = trans_coeff\n",
    "        self.holding_coeff = holding_coeff\n",
    "        return\n",
    "        \n",
    "    def replicate_cost_coeff(self, num_stocks, lookahead):\n",
    "        trans_cost_repl = np.ones((num_stocks,lookahead))/100\n",
    "        holding_cost_repl = np.ones((num_stocks, lookahead))/100\n",
    "        self.trans_coeff = trans_cost_repl*self.trans_coeff\n",
    "        self.holding_coeff = holding_cost_repl*self.holding_coeff\n",
    "        return\n",
    "    \n",
    "    def set_holding_cost(self, weights_new):\n",
    "        self.holding_cost += cp.sum(cp.multiply(self.holding_coeff, cp.neg(weights_new)))\n",
    "        return\n",
    "        \n",
    "    def calc_trans_cost(self, weights_new, weights_old, trans_coeff):\n",
    "        abs_trade= cp.abs(weights_new-weights_old)\n",
    "        return cp.sum(cp.multiply(trans_coeff, abs_trade))       \n",
    "    \n",
    "    \n",
    "    def set_trans_cost(self, weights_new,weights_old):\n",
    "        \n",
    "        weights_curr= weights_new[:,0]\n",
    "        if weights_new.shape[1]>1:         \n",
    "            weights_future = weights_new[:,1:]\n",
    "            weights_future_shift = weights_new[:,:-1]\n",
    "            self.trans_cost = self.calc_trans_cost(weights_future, weights_future_shift, self.trans_coeff[:,1:])\n",
    "\n",
    "        self.trans_cost += self.calc_trans_cost(weights_curr, weights_old,self.trans_coeff[:,0])\n",
    "        return\n",
    "\n",
    "\n",
    "class Constraints:\n",
    "        #List of all constraints\n",
    "        def __init__(self, constr_list=['cardinality','asset_limit_cardinality','no_short'], \n",
    "                     upper_limit=0.3, lower_limit=-0.3, stock_limit=15):\n",
    "            self.upper_limit = upper_limit\n",
    "            self.lower_limit = lower_limit\n",
    "            self.stock_limit = stock_limit\n",
    "            self.constr_list = constr_list\n",
    "            self.value=[]\n",
    "\n",
    "        \n",
    "        def set_constraints(self, all_weights, y):\n",
    "            # Set weight unity\n",
    "            weights=all_weights[:-1,:]\n",
    "            \n",
    "            self.value += [cp.sum(all_weights,axis=0)==1]\n",
    "            self.value += [all_weights[-1,:]>=0]\n",
    "            \n",
    "            if \"cardinality\" in self.constr_list:\n",
    "                self.value+= [cp.sum(y,axis=0)== self.stock_limit]   \n",
    "                \n",
    "            if \"no_short\" in self.constr_list:\n",
    "                self.value+=  [weights>=0]\n",
    "                \n",
    "            if \"asset_limit_cardinality\" in self.constr_list:\n",
    "                cardinality_upper_limit= cp.multiply(self.upper_limit, y)\n",
    "                cardinality_lower_limit = cp.multiply(self.lower_limit,y)\n",
    "                self.value+=  [weights>=cardinality_lower_limit, weights<=cardinality_upper_limit]\n",
    "\n",
    "            elif \"asset_limit\" in self.constr_list:\n",
    "                self.value+=[weights<=self.upper_limit, weights>=self.lower_limit]\n",
    "        \n",
    "            return\n",
    "\n",
    "\n",
    "class Risks:\n",
    "    def __init__(self, risk_type=\"MVO\", conf_lvl=0):\n",
    "        self.value=0\n",
    "        self.risk_type=risk_type\n",
    "        self.conf_lvl=conf_lvl\n",
    "        return\n",
    "        \n",
    "    def set_risk(self, weights, Q, lookahead):\n",
    "        portfolio_risk=0\n",
    "        robustness_cost=0\n",
    "        num_stocks = weights.shape[1]\n",
    "        \n",
    "        for i in range(lookahead):\n",
    "            portfolio_risk += cp.quad_form(weights[:,i], Q[i])        \n",
    "        self.value = portfolio_risk\n",
    "        \n",
    "        if self.risk_type == \"rect\":\n",
    "            for i in range(lookahead):\n",
    "                delta = stats.norm.ppf(self.conf_lvl)*np.sqrt(np.diag(Q[i]/num_stocks))\n",
    "                robustness_cost += delta@cp.abs(weights[:,i])\n",
    "            self.value += robustness_cost\n",
    "        \n",
    "        elif self.risk_type == \"ellip\":\n",
    "            for i in range(lookahead):\n",
    "         \n",
    "                penalty = cp.norm(np.sqrt(np.diag(Q[i]/num_stocks))@weights[:,i],2)\n",
    "                robustness_cost += stats.chi2.ppf(self.conf_lvl, num_stocks)*penalty\n",
    "            self.value += robustness_cost\n",
    "            \n",
    "        elif self.risk_type == \"cvar\":\n",
    "            pass\n",
    "        \n",
    "        elif self.risk_type == 'B-L':\n",
    "            self.value = 0\n",
    "            pass\n",
    "    \n",
    "        return\n",
    "\n",
    "    def get_RP_objective(self, weights, args):\n",
    "        Q = args[0]\n",
    "        assets_risk_budget = args[1]\n",
    "        lookahead = args[2]\n",
    "        cost_model = args[3]\n",
    "        \n",
    "        num_stocks = len(assets_risk_budget)\n",
    "\n",
    "        self.value=0\n",
    "        # We convert the weights to a matrix\n",
    "        weights = np.matrix(weights)\n",
    "        for i in range(lookahead):\n",
    "            # We calculate the risk of the weights distribution\n",
    "\n",
    "            portfolio_risk = np.sqrt((weights[0,num_stocks*i:num_stocks*(i+1)] * Q[i] \n",
    "                                      * weights[0,num_stocks*i:num_stocks*(i+1)].T))[0, 0]\n",
    "\n",
    "            # We calculate the contribution of each asset to the risk of the weights\n",
    "            # distribution\n",
    "            assets_risk_contribution = np.multiply(weights[0,num_stocks*i:num_stocks*(i+1)].T, Q[i] \n",
    "                                                   * weights[0,num_stocks*i:num_stocks*(i+1)].T)/ portfolio_risk\n",
    "\n",
    "            # We calculate the desired contribution of each asset to the risk of the\n",
    "            # weights distribution\n",
    "            assets_risk_target = np.asmatrix(np.multiply(portfolio_risk, assets_risk_budget))\n",
    "\n",
    "            # Error between the desired contribution and the calculated contribution of\n",
    "            # each asset\n",
    "            self.value += np.sum(np.square(assets_risk_contribution - assets_risk_target.T))\n",
    "            \n",
    "            # Get the holding costs\n",
    "            self.value += np.sum(cost_model.holding_coeff[0,0]*weights[0,num_stocks*i:num_stocks*(i+1)])\n",
    "            \n",
    "            # Get the transaction costs\n",
    "            if i < lookahead-1:\n",
    "                abs_trade = np.abs(weights[0, num_stocks*i:num_stocks*(i+1)]-\n",
    "                                   weights[0, num_stocks*(i+1):num_stocks*(i+2)])\n",
    "                self.value += np.sum(cost_model.trans_coeff[0,0]*abs_trade)\n",
    "            \n",
    "        # It returns the calculated error\n",
    "        return self.value \n",
    "    \n",
    "    \n",
    "    \n",
    "class Model:\n",
    "    def __init__(self, lam):\n",
    "        self.opt_weights = 0\n",
    "        self.status = None\n",
    "        self.lam = lam\n",
    "\n",
    "        return\n",
    "        \n",
    "    def MVO(self, port, mu , Q, rf, look_ahead, constr_model, cost_model, risk_model):\n",
    "        \n",
    "        mu_np = np.array(mu)\n",
    "        Q_np = np.array(Q)\n",
    "        \n",
    "        num_stocks = port.weights.shape[1]-1\n",
    "        \n",
    "        #Construct optimization problem\n",
    "        all_weights = cp.Variable((num_stocks+1,look_ahead))   \n",
    "        y = cp.Variable((num_stocks,look_ahead), boolean=True)\n",
    "        \n",
    "        weights_prev= port.weights[-1,:-1]   \n",
    "        weights=all_weights[:-1,:]\n",
    "\n",
    "        # Set model parameters\n",
    "        cost_model.set_trans_cost(weights, weights_prev)\n",
    "        cost_model.set_holding_cost(weights)    \n",
    "        constr_model.set_constraints(all_weights, y)\n",
    "        risk_model.set_risk(weights, Q,look_ahead)\n",
    "\n",
    "        # Get portfolio return\n",
    "        portfolio_return_per_period = mu_np@weights\n",
    "        rf_return = cp.sum(rf*all_weights[-1,:])\n",
    "        portfolio_return = cp.trace(portfolio_return_per_period)+rf_return\n",
    "        \n",
    "        objective= cp.Minimize(risk_model.value)\n",
    "        constr_model.value+= [portfolio_return>=self.lam]\n",
    "        \n",
    "        #Construct Problem and Solve\n",
    "        prob= cp.Problem(objective, constr_model.value)\n",
    "        result=prob.solve(solver=\"GUROBI\")\n",
    "        self.status= prob.status\n",
    "        if self.status == \"optimal\":\n",
    "            self.opt_weights=np.array(all_weights.value)[:,0]\n",
    "            print(\"port value period:\", portfolio_return_per_period.value)\n",
    "            print(\"port return:\",portfolio_return.value)\n",
    "            print(\"risk value:\",risk_model.value.value)\n",
    "            print(\"holding cost:\",cost_model.holding_cost.value)\n",
    "            print(\"trans cost:\", cost_model.trans_cost.value)\n",
    "    \n",
    "        else:\n",
    "            print(\"Unsolvable, use old weights\")\n",
    "            self.opt_weights=port.weights[-1].T\n",
    "     \n",
    "        return self.opt_weights\n",
    "    \n",
    "    def BL(self):\n",
    "        return\n",
    "\n",
    "\n",
    "    def risk_parity(self, port, Q, lookahead, risk_model, cost_model):\n",
    "        TOLERANCE = 1e-7\n",
    "        Q_np =np.array(Q)\n",
    "        num_stocks=port.weights.shape[1]-1\n",
    "\n",
    "        #Construct optimization problem\n",
    "        init_weights = np.tile(port.weights[-1,:-1],lookahead).astype(float)\n",
    "        init_rf = port.weights[-1,-1]\n",
    "        weight_total = 1-init_rf\n",
    "        \n",
    "        if np.count_nonzero(init_weights)==0:\n",
    "            init_weights = np.array([1/num_stocks]*num_stocks*lookahead)\n",
    "            weight_total=1\n",
    "            init_rf = 0\n",
    "\n",
    "        print(weight_total, init_weights)\n",
    "        \n",
    "        # The desired contribution of each asset to the portfolio risk: we want all\n",
    "        # assets to contribute equally\n",
    "        assets_risk_budget = [1/num_stocks] * num_stocks\n",
    "\n",
    "        # Optimisation process of weights\n",
    "        # Restrictions to consider in the optimisation: only long positions whose\n",
    "        # sum equals 100%\n",
    "        constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - weight_total},\n",
    "                       {'type': 'ineq', 'fun': lambda x: x})\n",
    "\n",
    "        # Optimisation process in scipy\n",
    "        optimize_result = minimize(fun=risk_model.get_RP_objective,\n",
    "                                   x0=init_weights,\n",
    "                                   args=[Q, assets_risk_budget, lookahead, cost_model],\n",
    "                                   method='SLSQP',\n",
    "                                   constraints=constraints,\n",
    "                                   tol=TOLERANCE,\n",
    "                                   options={'disp': False, 'maxiter':5000}\n",
    "                                  )\n",
    "\n",
    "        # Recover the weights from the optimised object\n",
    "        weights = np.array(optimize_result.x) \n",
    "        \n",
    "        self.opt_weights = np.concatenate((weights[0:num_stocks], np.array([init_rf])))\n",
    "        return self.opt_weights        \n",
    "    \n",
    "    \n",
    "class Backtest:\n",
    "    def __init__(self, start_date, end_date, lookback, lookahead, period='M'):\n",
    "        self.rebal_freq = period\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.lookback = lookback\n",
    "        self.lookahead = lookahead\n",
    "        self.reb_dates = None\n",
    "        return\n",
    "        \n",
    "    \n",
    "    def run(self, data, portfolio, factor_model, optimizer, constr_model, cost_model, risk_model):      \n",
    "        stock_return= data.stock_returns      \n",
    "        self.reb_dates= np.array(data.stock_returns.loc[self.start_date:self.end_date].index)    \n",
    "        \n",
    "        for t in self.reb_dates:\n",
    "            mu, Q = factor_model.get_param_estimate(t, data)\n",
    "            new_rf_rate=float(data.risk_free.loc[t])\n",
    "            if risk_model.risk_type in ('MVO', 'ellip', 'rect', 'cvar'):\n",
    "                weights = optimizer.MVO(portfolio, mu , Q, new_rf_rate, self.lookahead, constr_model, cost_model, risk_model)     \n",
    "            \n",
    "            elif risk_model.risk_type == 'B-L':\n",
    "                weights = optimizer.BL()\n",
    "            \n",
    "            elif risk_model.risk_type == 'risk-parity':\n",
    "                weights = optimizer.risk_parity(portfolio, Q, self.lookahead, risk_model, cost_model)\n",
    " \n",
    "                \n",
    "            portfolio.update_dates(t)\n",
    "            portfolio.update_weights(weights)\n",
    "            portfolio.update_returns(np.dot(weights[:-1],stock_return.loc[t])+weights[-1]*new_rf_rate)\n",
    "\n",
    "            ##How the lambdas influence\n",
    "            ##When to use MVO.. when to use CVaR... implement a CVaR\n",
    "\n",
    "        return portfolio.get_Sharpe(data)\n",
    "\n",
    "\n",
    "    def grid_search(self, data, portfolio, model, trans_coeff=0.2, hold_coeff=0.2, lam=0.9, conf_level=0.95):\n",
    "\n",
    "#         # Overall - currently test values are used\n",
    "#         pot_lookaheads = [1, 3, 6, 12, 60]\n",
    "#         pot_lookbacks = [2, 3, 6, 12, 60]\n",
    "\n",
    "#         # Factor Models\n",
    "#         factor_models = ['CAPM', 'FF', 'Carhart', 'PCA'] # Data\n",
    "#         regressions = ['linear', 'lasso', 'ridge', 'SVR'] # FactorModel\n",
    "\n",
    "#         # Constraints\n",
    "#         cardinalities = ['', 'cardinality']\n",
    "#         asset_limits = ['asset_limit_cardinality', 'asset_limit']\n",
    "#         no_shorts = ['', 'no_short']\n",
    "#         constraints_list = [cardinalities, asset_limits, no_shorts]\n",
    "\n",
    "#         stock_limits = list(range(5, 501, 5))\n",
    "\n",
    "#         # Optimization\n",
    "#         MVO_robustness = ['', 'rectangular', 'elliptical']\n",
    "\n",
    "        # Overall\n",
    "        pot_lookaheads = [5]\n",
    "        pot_lookbacks = [20]\n",
    "\n",
    "        # Factor Models\n",
    "        factor_models = ['FF', 'PCA']  # Data\n",
    "        weights = [[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1],[0.25,0.25,0.25,0.25],\n",
    "                       [0,0.5,0.5,0],[0,0.25,0.25,0.5],[0,0,0.5,0.5]]\n",
    "\n",
    "        # Constraints\n",
    "        cardinalities = ['cardinality']\n",
    "        asset_limits = ['asset_limit_cardinality', 'asset_limit']\n",
    "        no_shorts = ['no_short']\n",
    "        constraints_list = [cardinalities, asset_limits, no_shorts]\n",
    "\n",
    "        stock_limits = list(range(5, 21, 5))\n",
    "        upper_asset_limits = [1]\n",
    "        lower_asset_limits = [-1]\n",
    "\n",
    "        # Optimization\n",
    "        MVO_robustness = ['ellip']\n",
    "\n",
    "        # list of sharpe ratios per parameter combination\n",
    "        sharpe_ratios = []\n",
    "\n",
    "        # list of parameter combinations corresponding to sharpe ratio\n",
    "        parameter_combos = []\n",
    "\n",
    "        for combo in tqdm(list(itertools.product(factor_models, weights, \\\n",
    "                                                 list(itertools.product(*constraints_list)), stock_limits, \\\n",
    "                                                 upper_asset_limits, lower_asset_limits, MVO_robustness))):\n",
    "\n",
    "            # Store the combination\n",
    "            curr_combo = {'rebalance_freq': 'M', 'factor_model': combo[0], 'weights': combo[1],\n",
    "                          'constraints_list': list(combo[2]), 'stock_limit': combo[3], 'upper_asset_limit': combo[4],\n",
    "                          'lower_asset_limit': combo[5], 'robustness': combo[6]}\n",
    "\n",
    "            # Initial Setup\n",
    "            data.set_factor_returns(curr_combo['factor_model'], curr_combo['rebalance_freq'])\n",
    "\n",
    "            num_stocks = data.get_num_stocks()\n",
    "            cost_model = Costs(trans_coeff, hold_coeff)\n",
    "\n",
    "            # Get lookaheads that are multiples of the rebalancing frequency and <= 60 months\n",
    "            if curr_combo['rebalance_freq'] == 'M':\n",
    "                first = 1\n",
    "            else:\n",
    "                first = int(curr_combo['rebalance_freq'][0])\n",
    "\n",
    "            lookaheads = list(itertools.compress(pot_lookaheads, [look >= first for look in pot_lookaheads]))\n",
    "            lookbacks = list(itertools.compress(pot_lookbacks, [look >= first for look in pot_lookbacks]))\n",
    "\n",
    "            for lookahead in lookaheads:\n",
    "                curr_combo['lookahead'] = lookahead\n",
    "                for lookback in lookbacks:\n",
    "                    curr_combo['lookback'] = lookback\n",
    "\n",
    "                    # Continue Setup\n",
    "                    cost_model.replicate_cost_coeff(num_stocks, lookahead)\n",
    "                    constr_model = Constraints(curr_combo['constraints_list'])\n",
    "\n",
    "                    risk_model = Risks(curr_combo['robustness'], conf_level)\n",
    "\n",
    "                    # Run backtest\n",
    "                    factor = FactorModel(curr_combo['lookahead'], curr_combo['lookback'],\n",
    "                                         curr_combo['weights'])\n",
    "                    sharpe = self.run(data, portfolio, factor, model, constr_model, cost_model, risk_model)\n",
    "\n",
    "                    # Update results\n",
    "                    sharpe_ratios.append(sharpe)\n",
    "                    parameter_combos.append(curr_combo)\n",
    "\n",
    "        return sharpe_ratios, parameter_combos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n",
      " 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n",
      " 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n",
      " 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n",
      " 0.05 0.05 0.05 0.05]\n",
      "1.0 [0.01636673 0.01670439 0.01668813 0.01672048 0.01672069 0.01671952\n",
      " 0.01670358 0.01672125 0.0167203  0.01670598 0.01668414 0.0166616\n",
      " 0.01671814 0.01672132 0.01671554 0.01671933 0.01671806 0.01672132\n",
      " 0.01672036 0.01671836 0.01636673 0.01670439 0.01668813 0.01672048\n",
      " 0.01672069 0.01671952 0.01670358 0.01672125 0.0167203  0.01670598\n",
      " 0.01668414 0.0166616  0.01671814 0.01672132 0.01671554 0.01671933\n",
      " 0.01671806 0.01672132 0.01672036 0.01671836 0.01636673 0.01670439\n",
      " 0.01668813 0.01672048 0.01672069 0.01671952 0.01670358 0.01672125\n",
      " 0.0167203  0.01670598 0.01668414 0.0166616  0.01671814 0.01672132\n",
      " 0.01671554 0.01671933 0.01671806 0.01672132 0.01672036 0.01671836]\n",
      "1.0 [0.00832793 0.01701943 0.01672339 0.01719461 0.01719156 0.01717114\n",
      " 0.01701084 0.01719481 0.01717498 0.01711318 0.01692505 0.0163399\n",
      " 0.01718671 0.01717741 0.01719503 0.01716071 0.01716509 0.01718814\n",
      " 0.01717564 0.01716867 0.00832793 0.01701943 0.01672339 0.01719461\n",
      " 0.01719156 0.01717114 0.01701084 0.01719481 0.01717498 0.01711318\n",
      " 0.01692505 0.0163399  0.01718671 0.01717741 0.01719503 0.01716071\n",
      " 0.01716509 0.01718814 0.01717564 0.01716867 0.00832793 0.01701943\n",
      " 0.01672339 0.01719461 0.01719156 0.01717114 0.01701084 0.01719481\n",
      " 0.01717498 0.01711318 0.01692505 0.0163399  0.01718671 0.01717741\n",
      " 0.01719503 0.01716071 0.01716509 0.01718814 0.01717564 0.01716867]\n",
      "1.0 [0.00836593 0.01705289 0.0167497  0.01723901 0.0172362  0.01721592\n",
      " 0.01704786 0.01723775 0.01721439 0.01715803 0.01695841 0.01635585\n",
      " 0.01723136 0.01721927 0.01723655 0.01720574 0.01721011 0.01723076\n",
      " 0.01722064 0.0172137  0.00836593 0.01705289 0.0167497  0.01723901\n",
      " 0.0172362  0.01721592 0.01704786 0.01723775 0.01721439 0.01715803\n",
      " 0.01695841 0.01635585 0.01723136 0.01721927 0.01723655 0.01720574\n",
      " 0.01721011 0.01723076 0.01722064 0.0172137  0.00836593 0.01705289\n",
      " 0.0167497  0.01723901 0.0172362  0.01721592 0.01704786 0.01723775\n",
      " 0.01721439 0.01715803 0.01695841 0.01635585 0.01723136 0.01721927\n",
      " 0.01723655 0.01720574 0.01721011 0.01723076 0.01722064 0.0172137 ]\n",
      "1.0 [0.00834468 0.01702262 0.01671602 0.01722344 0.01722101 0.01720098\n",
      " 0.01702484 0.01722159 0.01719391 0.01714305 0.01693322 0.01631368\n",
      " 0.01721592 0.01720145 0.01722023 0.01719085 0.01719523 0.01721358\n",
      " 0.01720575 0.01719882 0.00834468 0.01702262 0.01671602 0.01722344\n",
      " 0.01722101 0.01720098 0.01702484 0.01722159 0.01719391 0.01714305\n",
      " 0.01693322 0.01631368 0.01721592 0.01720145 0.01722023 0.01719085\n",
      " 0.01719523 0.01721358 0.01720575 0.01719882 0.00834468 0.01702262\n",
      " 0.01671602 0.01722344 0.01722101 0.01720098 0.01702484 0.01722159\n",
      " 0.01719391 0.01714305 0.01693322 0.01631368 0.01721592 0.01720145\n",
      " 0.01722023 0.01719085 0.01719523 0.01721358 0.01720575 0.01719882]\n",
      "1.0 [0.0083513  0.01702266 0.01671453 0.01723802 0.01723593 0.01721595\n",
      " 0.0170329  0.01723505 0.01720038 0.01715813 0.01693378 0.01630235\n",
      " 0.01723094 0.01721611 0.01723443 0.01720607 0.01720964 0.01722628\n",
      " 0.01722074 0.01721399 0.0083513  0.01702266 0.01671453 0.01723802\n",
      " 0.01723593 0.01721595 0.0170329  0.01723505 0.01720038 0.01715813\n",
      " 0.01693378 0.01630235 0.01723094 0.01721611 0.01723443 0.01720607\n",
      " 0.01720964 0.01722628 0.01722074 0.01721399 0.0083513  0.01702266\n",
      " 0.01671453 0.01723802 0.01723593 0.01721595 0.0170329  0.01723505\n",
      " 0.01720038 0.01715813 0.01693378 0.01630235 0.01723094 0.01721611\n",
      " 0.01723443 0.01720607 0.01720964 0.01722628 0.01722074 0.01721399]\n",
      "1.0 [0.00834558 0.01701711 0.01669856 0.0172376  0.0172361  0.01721604\n",
      " 0.01702388 0.01723156 0.01719312 0.01715838 0.01691624 0.01627606\n",
      " 0.01723073 0.01721505 0.01723348 0.01720614 0.01720693 0.01722352\n",
      " 0.01722071 0.01721423 0.00834558 0.01701711 0.01669856 0.0172376\n",
      " 0.0172361  0.01721604 0.01702388 0.01723156 0.01719312 0.01715838\n",
      " 0.01691624 0.01627606 0.01723073 0.01721505 0.01723348 0.01720614\n",
      " 0.01720693 0.01722352 0.01722071 0.01721423 0.00834558 0.01701711\n",
      " 0.01669856 0.0172376  0.0172361  0.01721604 0.01702388 0.01723156\n",
      " 0.01719312 0.01715838 0.01691624 0.01627606 0.01723073 0.01721505\n",
      " 0.01723348 0.01720614 0.01720693 0.01722352 0.01722071 0.01721423]\n",
      "1.0 [0.00834939 0.01702072 0.01668846 0.01724412 0.01724387 0.0172237\n",
      " 0.0170193  0.01723636 0.01719445 0.01716617 0.01689887 0.0162596\n",
      " 0.01723813 0.01722148 0.01724024 0.01721394 0.0172118  0.01722853\n",
      " 0.01722845 0.01722205 0.00834939 0.01702072 0.01668846 0.01724412\n",
      " 0.01724387 0.0172237  0.0170193  0.01723636 0.01719445 0.01716617\n",
      " 0.01689887 0.0162596  0.01723813 0.01722148 0.01724024 0.01721394\n",
      " 0.0172118  0.01722853 0.01722845 0.01722205 0.00834939 0.01702072\n",
      " 0.01668846 0.01724412 0.01724387 0.0172237  0.0170193  0.01723636\n",
      " 0.01719445 0.01716617 0.01689887 0.0162596  0.01723813 0.01722148\n",
      " 0.01724024 0.01721394 0.0172118  0.01722853 0.01722845 0.01722205]\n",
      "1.0 [0.00835213 0.01702061 0.01667235 0.01724719 0.01724795 0.01722769\n",
      " 0.01701168 0.01723819 0.01719237 0.01717034 0.01687432 0.0162385\n",
      " 0.01724203 0.01722509 0.01724315 0.0172181  0.01721269 0.01723026\n",
      " 0.01723249 0.01722624 0.00835213 0.01702061 0.01667235 0.01724719\n",
      " 0.01724795 0.01722769 0.01701168 0.01723819 0.01719237 0.01717034\n",
      " 0.01687432 0.0162385  0.01724203 0.01722509 0.01724315 0.0172181\n",
      " 0.01721269 0.01723026 0.01723249 0.01722624 0.00835213 0.01702061\n",
      " 0.01667235 0.01724719 0.01724795 0.01722769 0.01701168 0.01723819\n",
      " 0.01719237 0.01717034 0.01687432 0.0162385  0.01724203 0.01722509\n",
      " 0.01724315 0.0172181  0.01721269 0.01723026 0.01723249 0.01722624]\n",
      "1.0 [0.0081372  0.0169863  0.01670874 0.01725716 0.01723048 0.01729963\n",
      " 0.01708184 0.01727803 0.01715503 0.0166278  0.01685317 0.01623024\n",
      " 0.01717627 0.01696062 0.01720905 0.01750429 0.01721201 0.01768277\n",
      " 0.01728315 0.01696537 0.0081372  0.0169863  0.01670874 0.01725716\n",
      " 0.01723048 0.01729963 0.01708184 0.01727803 0.01715503 0.0166278\n",
      " 0.01685317 0.01623024 0.01717627 0.01696062 0.01720905 0.01750429\n",
      " 0.01721201 0.01768277 0.01728315 0.01696537 0.0081372  0.0169863\n",
      " 0.01670874 0.01725716 0.01723048 0.01729963 0.01708184 0.01727803\n",
      " 0.01715503 0.0166278  0.01685317 0.01623024 0.01717627 0.01696062\n",
      " 0.01720905 0.01750429 0.01721201 0.01768277 0.01728315 0.01696537]\n",
      "1.0 [0.00817684 0.01702438 0.01674165 0.01729692 0.01727232 0.01734191\n",
      " 0.01711565 0.01731448 0.01718851 0.01667025 0.01686312 0.01625181\n",
      " 0.01721788 0.01700236 0.01725046 0.0175465  0.01725073 0.01772089\n",
      " 0.01732535 0.01700736 0.00817684 0.01702438 0.01674165 0.01729692\n",
      " 0.01727232 0.01734191 0.01711565 0.01731448 0.01718851 0.01667025\n",
      " 0.01686312 0.01625181 0.01721788 0.01700236 0.01725046 0.0175465\n",
      " 0.01725073 0.01772089 0.01732535 0.01700736 0.00817684 0.01702438\n",
      " 0.01674165 0.01729692 0.01727232 0.01734191 0.01711565 0.01731448\n",
      " 0.01718851 0.01667025 0.01686312 0.01625181 0.01721788 0.01700236\n",
      " 0.01725046 0.0175465  0.01725073 0.01772089 0.01732535 0.01700736]\n",
      "1.0 [0.00816026 0.01700606 0.01671724 0.01728119 0.01725818 0.01732824\n",
      " 0.01709399 0.0172918  0.01716632 0.01665674 0.01682819 0.01621582\n",
      " 0.01720251 0.01698771 0.01723596 0.01753276 0.01723398 0.01770595\n",
      " 0.01731152 0.01699338 0.00816026 0.01700606 0.01671724 0.01728119\n",
      " 0.01725818 0.01732824 0.01709399 0.0172918  0.01716632 0.01665674\n",
      " 0.01682819 0.01621582 0.01720251 0.01698771 0.01723596 0.01753276\n",
      " 0.01723398 0.01770595 0.01731152 0.01699338 0.00816026 0.01700606\n",
      " 0.01671724 0.01728119 0.01725818 0.01732824 0.01709399 0.0172918\n",
      " 0.01716632 0.01665674 0.01682819 0.01621582 0.01720251 0.01698771\n",
      " 0.01723596 0.01753276 0.01723398 0.01770595 0.01731152 0.01699338]\n",
      "1.0 [0.00817176 0.01701327 0.01672256 0.01729224 0.01727215 0.01734212\n",
      " 0.01709963 0.01729705 0.01717279 0.0166708  0.0168249  0.01620933\n",
      " 0.01721474 0.01700043 0.01724874 0.01754666 0.0172439  0.01771696\n",
      " 0.0173253  0.01700732 0.00817176 0.01701327 0.01672256 0.01729224\n",
      " 0.01727215 0.01734212 0.01709963 0.01729705 0.01717279 0.0166708\n",
      " 0.0168249  0.01620933 0.01721474 0.01700043 0.01724874 0.01754666\n",
      " 0.0172439  0.01771696 0.0173253  0.01700732 0.00817176 0.01701327\n",
      " 0.01672256 0.01729224 0.01727215 0.01734212 0.01709963 0.01729705\n",
      " 0.01717279 0.0166708  0.0168249  0.01620933 0.01721474 0.01700043\n",
      " 0.01724874 0.01754666 0.0172439  0.01771696 0.0173253  0.01700732]\n",
      "1.0 [0.00817531 0.01701029 0.01671266 0.01728899 0.01727122 0.01734192\n",
      " 0.0170911  0.01728949 0.01716581 0.01667078 0.01680648 0.01619307\n",
      " 0.01721283 0.01699902 0.01723999 0.01754645 0.01723979 0.01771464\n",
      " 0.01732485 0.01700721 0.00817531 0.01701029 0.01671266 0.01728899\n",
      " 0.01727122 0.01734192 0.0170911  0.01728949 0.01716581 0.01667078\n",
      " 0.01680648 0.01619307 0.01721283 0.01699902 0.01723999 0.01754645\n",
      " 0.01723979 0.01771464 0.01732485 0.01700721 0.00817531 0.01701029\n",
      " 0.01671266 0.01728899 0.01727122 0.01734192 0.0170911  0.01728949\n",
      " 0.01716581 0.01667078 0.01680648 0.01619307 0.01721283 0.01699902\n",
      " 0.01723999 0.01754645 0.01723979 0.01771464 0.01732485 0.01700721]\n",
      "1.0 [0.00818734 0.01701568 0.01671463 0.01729468 0.01727809 0.01734962\n",
      " 0.01709653 0.01729106 0.01716568 0.01667885 0.01675825 0.01619269\n",
      " 0.0172199  0.01700615 0.01723471 0.01755407 0.01724265 0.01772113\n",
      " 0.01733263 0.01701512 0.00818734 0.01701568 0.01671463 0.01729468\n",
      " 0.01727809 0.01734962 0.01709653 0.01729106 0.01716568 0.01667885\n",
      " 0.01675825 0.01619269 0.0172199  0.01700615 0.01723471 0.01755407\n",
      " 0.01724265 0.01772113 0.01733263 0.01701512 0.00818734 0.01701568\n",
      " 0.01671463 0.01729468 0.01727809 0.01734962 0.01709653 0.01729106\n",
      " 0.01716568 0.01667885 0.01675825 0.01619269 0.0172199  0.01700615\n",
      " 0.01723471 0.01755407 0.01724265 0.01772113 0.01733263 0.01701512]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 [0.0081974  0.01701911 0.01670073 0.01729887 0.01728302 0.01735495\n",
      " 0.01710059 0.01729175 0.01714927 0.01668436 0.01670304 0.01619164\n",
      " 0.01722479 0.01701112 0.01722814 0.01755934 0.01724405 0.01772536\n",
      " 0.01733789 0.01702042 0.0081974  0.01701911 0.01670073 0.01729887\n",
      " 0.01728302 0.01735495 0.01710059 0.01729175 0.01714927 0.01668436\n",
      " 0.01670304 0.01619164 0.01722479 0.01701112 0.01722814 0.01755934\n",
      " 0.01724405 0.01772536 0.01733789 0.01702042 0.0081974  0.01701911\n",
      " 0.01670073 0.01729887 0.01728302 0.01735495 0.01710059 0.01729175\n",
      " 0.01714927 0.01668436 0.01670304 0.01619164 0.01722479 0.01701112\n",
      " 0.01722814 0.01755934 0.01724405 0.01772536 0.01733789 0.01702042]\n",
      "1.0 [0.00820933 0.01702245 0.01668668 0.01730489 0.01728974 0.01736199\n",
      " 0.01710714 0.0172914  0.01713526 0.01669157 0.0166503  0.01619543\n",
      " 0.01723145 0.01701802 0.01722378 0.0175661  0.01724763 0.01773124\n",
      " 0.01734482 0.01702751 0.00820933 0.01702245 0.01668668 0.01730489\n",
      " 0.01728974 0.01736199 0.01710714 0.0172914  0.01713526 0.01669157\n",
      " 0.0166503  0.01619543 0.01723145 0.01701802 0.01722378 0.0175661\n",
      " 0.01724763 0.01773124 0.01734482 0.01702751 0.00820933 0.01702245\n",
      " 0.01668668 0.01730489 0.01728974 0.01736199 0.01710714 0.0172914\n",
      " 0.01713526 0.01669157 0.0166503  0.01619543 0.01723145 0.01701802\n",
      " 0.01722378 0.0175661  0.01724763 0.01773124 0.01734482 0.01702751]\n",
      "1.0 [0.00822376 0.01702782 0.01667895 0.01731105 0.01729521 0.01736994\n",
      " 0.01711492 0.01729435 0.01712761 0.01669971 0.01656816 0.01617767\n",
      " 0.01723948 0.01702609 0.01722366 0.01757401 0.01725208 0.0177386\n",
      " 0.0173529  0.01703555 0.00822376 0.01702782 0.01667895 0.01731105\n",
      " 0.01729521 0.01736994 0.01711492 0.01729435 0.01712761 0.01669971\n",
      " 0.01656816 0.01617767 0.01723948 0.01702609 0.01722366 0.01757401\n",
      " 0.01725208 0.0177386  0.0173529  0.01703555 0.00822376 0.01702782\n",
      " 0.01667895 0.01731105 0.01729521 0.01736994 0.01711492 0.01729435\n",
      " 0.01712761 0.01669971 0.01656816 0.01617767 0.01723948 0.01702609\n",
      " 0.01722366 0.01757401 0.01725208 0.0177386  0.0173529  0.01703555]\n",
      "1.0 [0.00823838 0.01703364 0.01667358 0.01731728 0.01730089 0.0173783\n",
      " 0.01712336 0.01729778 0.01712051 0.01670815 0.01648356 0.01616431\n",
      " 0.01724781 0.01703456 0.0172273  0.01758228 0.0172537  0.01774571\n",
      " 0.01736031 0.01704395 0.00823838 0.01703364 0.01667358 0.01731728\n",
      " 0.01730089 0.0173783  0.01712336 0.01729778 0.01712051 0.01670815\n",
      " 0.01648356 0.01616431 0.01724781 0.01703456 0.0172273  0.01758228\n",
      " 0.0172537  0.01774571 0.01736031 0.01704395 0.00823838 0.01703364\n",
      " 0.01667358 0.01731728 0.01730089 0.0173783  0.01712336 0.01729778\n",
      " 0.01712051 0.01670815 0.01648356 0.01616431 0.01724781 0.01703456\n",
      " 0.0172273  0.01758228 0.0172537  0.01774571 0.01736031 0.01704395]\n",
      "1.0 [0.00825303 0.01703746 0.01667299 0.01732427 0.01730625 0.01738622\n",
      " 0.01713167 0.01730105 0.01711617 0.01671647 0.01640064 0.01615425\n",
      " 0.01725609 0.01704297 0.01721689 0.01759042 0.01725568 0.01775095\n",
      " 0.01736754 0.01705222 0.00825303 0.01703746 0.01667299 0.01732427\n",
      " 0.01730625 0.01738622 0.01713167 0.01730105 0.01711617 0.01671647\n",
      " 0.01640064 0.01615425 0.01725609 0.01704297 0.01721689 0.01759042\n",
      " 0.01725568 0.01775095 0.01736754 0.01705222 0.00825303 0.01703746\n",
      " 0.01667299 0.01732427 0.01730625 0.01738622 0.01713167 0.01730105\n",
      " 0.01711617 0.01671647 0.01640064 0.01615425 0.01725609 0.01704297\n",
      " 0.01721689 0.01759042 0.01725568 0.01775095 0.01736754 0.01705222]\n"
     ]
    }
   ],
   "source": [
    "##Main\n",
    "\n",
    "#Set up Data\n",
    "price_data = pd.read_csv(\"../Data/sp500df.csv\", index_col='Date')\n",
    "rfr = pd.DataFrame({'risk_free': [0.01]*len(price_data.index)}, index = price_data.index)\n",
    "data_set = Data(price_data, rfr,[i for i in range(20)])\n",
    "data_set.set_factor_returns()\n",
    "\n",
    "#Set Up Portfolio\n",
    "num_stocks=data_set.get_num_stocks()\n",
    "port= Portfolio(data_set)\n",
    "\n",
    "#Set Up model\n",
    "start_date = \"2014-10-31\"\n",
    "end_date = \"2017-11-01\"\n",
    "lookback = 20\n",
    "lookahead = 3\n",
    "lam = 0.05\n",
    "trans_coeff = 0.1\n",
    "holding_coeff = 0.1\n",
    "conf_level = 0.99\n",
    "\n",
    "# Define constraints to use\n",
    "#constr_list = [ \"asset_limit\"]\n",
    "constr_list = [ \"cardinality\", \"asset_limit_cardinality\"]\n",
    "constr_model = Constraints(constr_list)\n",
    "\n",
    "cost_model = Costs(trans_coeff, holding_coeff)\n",
    "cost_model.replicate_cost_coeff(num_stocks, lookahead)\n",
    "\n",
    "opt_model = Model(lam)\n",
    "risk_model = Risks(\"risk-parity\", conf_level)\n",
    "\n",
    "regress_weighting = [0,0.5,0.5,0]\n",
    "factor_model = FactorModel(lookahead, lookback, regress_weighting)\n",
    "\n",
    "back_test_ex = Backtest(start_date, end_date, lookback, lookahead)\n",
    "back_test_ex.run(data_set, port, factor_model, opt_model, constr_model, cost_model, risk_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "port.weights[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "port.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# #Set up Data\n",
    "# price_data = pd.read_csv(\"../Data/sp500df.csv\", index_col='Date')\n",
    "# rfr = pd.DataFrame({'risk_free': [0.01]*len(price_data.index)}, index = price_data.index)\n",
    "# data_set = Data(price_data, rfr)\n",
    "# data_set.set_factor_returns()\n",
    "\n",
    "# #Set Up Portfolio and Model\n",
    "# port=Portfolio(data_set)\n",
    "\n",
    "# #Set Up model\n",
    "# start_date= \"2014-10-31\"\n",
    "# end_date= \"2017-11-01\"\n",
    "# lookback=20\n",
    "# lookahead=5\n",
    "# lam=0.9\n",
    "# model=Model(lam)\n",
    "\n",
    "# # Define constraints to use\n",
    "# test = Backtest(start_date, end_date, lookback, lookahead)\n",
    "# test.grid_search(data_set, port, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-9a895c8f6036>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mback_test_ex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBacktest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookahead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mback_test_ex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstr_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrisk_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-2e4f8349250b>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, portfolio, factor_model, optimizer, constr_model, cost_model, risk_model)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mrisk_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrisk_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'risk-parity'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m                 \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrisk_parity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mportfolio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookahead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrisk_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-2e4f8349250b>\u001b[0m in \u001b[0;36mrisk_parity\u001b[0;34m(self, port, Q, lookahead, risk_model, cost_model)\u001b[0m\n\u001b[1;32m    438\u001b[0m                                    \u001b[0mconstraints\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraints\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m                                    \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTOLERANCE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                                    \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'disp'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'maxiter'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m                                   )\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'slsqp'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m         return _minimize_slsqp(fun, x0, args, jac, bounds,\n\u001b[0;32m--> 618\u001b[0;31m                                constraints, callback=callback, **options)\n\u001b[0m\u001b[1;32m    619\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'trust-constr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         return _minimize_trustregion_constr(fun, x0, args, jac, hess, hessp,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/scipy/optimize/slsqp.py\u001b[0m in \u001b[0;36m_minimize_slsqp\u001b[0;34m(func, x0, args, jac, bounds, constraints, maxiter, ftol, iprint, disp, eps, callback, **unknown_options)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0;31m# Compute objective function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m             \u001b[0mfx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m                 \u001b[0mfx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-2e4f8349250b>\u001b[0m in \u001b[0;36mget_RP_objective\u001b[0;34m(self, weights, args)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;31m# Error between the desired contribution and the calculated contribution of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0;31m# each asset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massets_risk_contribution\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0massets_risk_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0;31m# Get the holding costs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "#Set up Data\n",
    "price_data = pd.read_csv(\"../Data/sp500df.csv\", index_col='Date')\n",
    "rfr = pd.DataFrame({'risk_free': [0.01]*len(price_data.index)}, index = price_data.index)\n",
    "data_set = Data(price_data, rfr, [i for i in range(100)])\n",
    "data_set.set_factor_returns()\n",
    "\n",
    "#Set Up Portfolio\n",
    "num_stocks=data_set.get_num_stocks()\n",
    "port= Portfolio(data_set)\n",
    "\n",
    "#Set Up model\n",
    "start_date = \"2014-10-31\"\n",
    "end_date = \"2017-11-01\"\n",
    "lookback = 20\n",
    "lookahead = 5\n",
    "lam = 0.9\n",
    "trans_coeff = 0\n",
    "holding_coeff = 0\n",
    "conf_level = 0.95\n",
    "\n",
    "# Define constraints to use\n",
    "constr_list = [\"no_short\", \"cardinality\", \"asset_limit_cardinality\"]\n",
    "constr_model = Constraints(constr_list)\n",
    "\n",
    "cost_model = Costs(trans_coeff, holding_coeff)\n",
    "cost_model.replicate_cost_coeff(num_stocks, lookahead)\n",
    "\n",
    "opt_model = Model(lam)\n",
    "risk_model = Risks(\"risk-parity\", conf_level)\n",
    "\n",
    "regress_weighting = [0,0.5,0.5,0]\n",
    "factor_model = FactorModel(lookahead, lookback, regress_weighting)\n",
    "\n",
    "back_test_ex = Backtest(start_date, end_date, lookback, lookahead)\n",
    "back_test_ex.run(data_set, port, factor_model, opt_model, constr_model, cost_model, risk_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "port.weights[36]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "port.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_weights()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "port.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorModel:\n",
    "    def __init__(self, lookahead, lookback, regress_weighting):\n",
    "        \n",
    "        \"\"\"\n",
    "        lookahead: number of periods in the future to estimate\n",
    "        lookback: number of periods in the past to use for estimations\n",
    "        regress_weighting: array of size 4 with weight corresponding to each regression type; adds up to 1; \n",
    "        order is linear, lasso, ridge, SVR; in the case where there is one 1 and the rest 0's, there is no ensembling;\n",
    "        can artifically call LSTM by setting all weights to 0\n",
    "        \"\"\"\n",
    "        self.lookahead = lookahead\n",
    "        self.lookback = lookback\n",
    "        self.regress_weighting = regress_weighting\n",
    "        return\n",
    "               \n",
    "    def get_param_estimate(self, rebal_date, data):\n",
    "               \n",
    "        if sum(self.regress_weighting) == 0:\n",
    "            return self.get_mu_LSTM(rebal_date, data)\n",
    "\n",
    "        elif sum(self.regress_weighting) == 1:\n",
    "            return self.get_mu_Q_regression(rebal_date, data)\n",
    "        \n",
    "        else:\n",
    "            return \"ERROR: This regression weighting is not valid. Please make sure the weights sum to 1. You can also give all zeros for LSTM.\"\n",
    "        \n",
    "    def get_mu_Q_regression(self, rebal_date, data): \n",
    "        returns_data = data.stock_returns\n",
    "        factor_data = data.factor_returns\n",
    "        lookahead = self.lookahead\n",
    "        lookback = self.lookback\n",
    "        regress_weighting = self.regress_weighting\n",
    "                \n",
    "        # For keeping track of mu's and Q's from each period\n",
    "        mu_arr = []\n",
    "        Q_arr = []\n",
    "\n",
    "        n_factors = len(factor_data.columns)\n",
    "\n",
    "        returns_data = data.get_lookback_data(returns_data, rebal_date, lookback)\n",
    "        factor_data = data.get_lookback_data(factor_data, rebal_date, lookback)\n",
    "        \n",
    "        for i in range(0, lookahead):\n",
    "\n",
    "            # Calculate the factor covariance matrix\n",
    "            F = factor_data.loc[:, factor_data.columns != 'Ones'].cov()\n",
    "\n",
    "            # Calculate the factor expected excess return from historical data using the geometric mean\n",
    "            factor_data['Ones'] = [1 for i in range(len(factor_data))]\n",
    "            gmean = stats.gmean(factor_data + 1,axis=0) - 1\n",
    "\n",
    "            # Set up X and Y to determine alpha and beta\n",
    "            X = factor_data\n",
    "            Y = returns_data\n",
    "            X = X.to_numpy()\n",
    "            Y = Y.to_numpy()\n",
    "\n",
    "            \n",
    "            ### LINEAR REGRESSION\n",
    "        \n",
    "            model = LinearRegression().fit(X,Y)\n",
    "            alpha = model.intercept_\n",
    "            beta = model.coef_[:,0:n_factors]\n",
    "\n",
    "            # Calculate the residuals \n",
    "            alpha = np.reshape(alpha,(alpha.size,1))\n",
    "            epsilon = returns_data.to_numpy() - np.matmul(X, np.transpose(np.hstack((beta, alpha))))\n",
    "\n",
    "            # Calculate the residual variance with \"N - p - 1\" degrees of freedom\n",
    "            sigmaEp = np.sum(epsilon**2, axis=0) / (len(returns_data) - n_factors - 1)\n",
    "\n",
    "            #  Calculate the asset expected excess returns\n",
    "            mu_linear = model.predict([gmean])[0]\n",
    "\n",
    "            # Calculate the diagonal matrix of residuals and the asset covariance matrix\n",
    "            D = np.diag(sigmaEp)\n",
    "\n",
    "            # Calculate the covariance matrix\n",
    "            Q_linear = np.matmul(np.matmul(beta,F.to_numpy()),beta.T)+D\n",
    "\n",
    "\n",
    "            ### LASSO REGRESSION\n",
    "\n",
    "            model = Lasso().fit(X,Y)\n",
    "            alpha = model.intercept_\n",
    "            beta = model.coef_[:,0:n_factors]\n",
    "\n",
    "            # Calculate the residuals \n",
    "            alpha = np.reshape(alpha,(alpha.size,1))\n",
    "            epsilon = returns_data.to_numpy() - np.matmul(X, np.transpose(np.hstack((beta, alpha))))\n",
    "\n",
    "            # Calculate the residual variance with \"N - p - 1\" degrees of freedom\n",
    "            sigmaEp = np.sum(epsilon**2, axis=0) / (len(returns_data) - n_factors - 1)\n",
    "\n",
    "            #  Calculate the asset expected excess returns\n",
    "            mu_lasso = model.predict([gmean])[0]\n",
    "\n",
    "            # Calculate the diagonal matrix of residuals and the asset covariance matrix\n",
    "            D = np.diag(sigmaEp)\n",
    "\n",
    "            # Calculate the covariance matrix\n",
    "            Q_lasso = np.matmul(np.matmul(beta,F.to_numpy()),beta.T)+D\n",
    "\n",
    "\n",
    "            ### RIDGE REGRESSION\n",
    "\n",
    "            model = Ridge().fit(X,Y)\n",
    "            alpha = model.intercept_\n",
    "            beta = model.coef_[:,0:n_factors]\n",
    "\n",
    "            # Calculate the residuals \n",
    "            alpha = np.reshape(alpha,(alpha.size,1))\n",
    "            epsilon = returns_data.to_numpy() - np.matmul(X, np.transpose(np.hstack((beta, alpha))))\n",
    "\n",
    "            # Calculate the residual variance with \"N - p - 1\" degrees of freedom\n",
    "            sigmaEp = np.sum(epsilon**2, axis=0) / (len(returns_data) - n_factors - 1)\n",
    "\n",
    "            #  Calculate the asset expected excess returns\n",
    "            mu_ridge = model.predict([gmean])[0]\n",
    "\n",
    "            # Calculate the diagonal matrix of residuals and the asset covariance matrix\n",
    "            D = np.diag(sigmaEp)\n",
    "\n",
    "            # Calculate the covariance matrix\n",
    "            Q_ridge = np.matmul(np.matmul(beta,F.to_numpy()),beta.T)+D\n",
    "\n",
    "\n",
    "            ### SUPPORT VECTOR REGRESSION\n",
    "\n",
    "            model = make_pipeline(StandardScaler(), MultiOutputRegressor(LinearSVR(C=1, dual=False, loss=\"squared_epsilon_insensitive\"))).fit(X, Y)\n",
    "            beta = np.array([[model.named_steps['multioutputregressor'].estimators_[i].coef_[0:n_factors] for i in range(len(model.named_steps['multioutputregressor'].estimators_))]])[0]\n",
    "            alpha = np.array([model.named_steps['multioutputregressor'].estimators_[i].intercept_[0] for i in range(len(model.named_steps['multioutputregressor'].estimators_))])\n",
    "\n",
    "            # Calculate the residuals \n",
    "            alpha = np.reshape(alpha,(alpha.size,1))\n",
    "            epsilon = returns_data.to_numpy() - np.matmul(X, np.transpose(np.hstack((beta, alpha))))\n",
    "\n",
    "            # Calculate the residual variance with \"N - p - 1\" degrees of freedom\n",
    "            sigmaEp = np.sum(epsilon**2, axis=0) / (len(returns_data) - n_factors - 1)\n",
    "\n",
    "            #  Calculate the asset expected excess returns\n",
    "            mu_SVR = model.predict([gmean])[0]\n",
    "\n",
    "            # Calculate the diagonal matrix of residuals and the asset covariance matrix\n",
    "            D = np.diag(sigmaEp)\n",
    "\n",
    "            # Calculate the covariance matrix\n",
    "            Q_SVR = np.matmul(np.matmul(beta,F.to_numpy()),beta.T)+D\n",
    "\n",
    "        \n",
    "            # Ensemble the methods\n",
    "            mu = regress_weighting[0]*mu_linear + regress_weighting[1]*mu_lasso + regress_weighting[2]*mu_ridge + regress_weighting[3]*mu_SVR\n",
    "            Q = regress_weighting[0]*Q_linear + regress_weighting[1]*Q_lasso + regress_weighting[2]*Q_ridge + regress_weighting[3]*Q_SVR\n",
    "\n",
    "            # Add mu and Q to array\n",
    "            mu_arr.append(mu)\n",
    "            Q_arr.append(Q)\n",
    "\n",
    "            # Update for next time step\n",
    "            factor_data = factor_data[1:]\n",
    "            factor_append = pd.Series(gmean, index = factor_data.columns)\n",
    "            factor_data = factor_data.append(factor_append, ignore_index=True)\n",
    "\n",
    "            returns_data = returns_data[1:]\n",
    "            mu_append = pd.Series(mu, index=returns_data.columns)\n",
    "            returns_data = returns_data.append(mu_append, ignore_index=True)   \n",
    "\n",
    "        return mu_arr, Q_arr\n",
    "        \n",
    "    def get_mu_LSTM(self, rebal_date, data): \n",
    "        returns_data = data.stock_returns\n",
    "        factor_data = data.factor_returns\n",
    "        \n",
    "        lookahead = self.lookahead\n",
    "        lookback = self.lookback\n",
    "        regress_weighting = self.regress_weighting\n",
    "\n",
    "        returns_data = data.get_lookback_data(returns_data, rebal_date, lookback)\n",
    "        factor_data = data.get_lookback_data(factor_data, rebal_date, lookback)\n",
    "        \n",
    "        tempx, tempy = self.generate_X_y(factor_data.values, returns_data.values, lookback, lookahead)\n",
    "        train_x, test_x, train_y, test_y = self.traintest_split(tempx, tempy)\n",
    "\n",
    "        # scale inputs\n",
    "        scaled_train_x = (train_x - train_x.min())/(train_x.max() - train_x.min())\n",
    "        scaled_test_x = (test_x - test_x.min())/(test_x.max() - test_x.min())\n",
    "        scaled_train_y = (train_y - train_y.min())/(train_y.max() - train_y.min())\n",
    "        scaled_test_y = (test_y - test_y.min())/(test_y.max() - test_y.min())\n",
    "\n",
    "        mu = self.get_prediction(train_x, train_y, factor_data, lookback)\n",
    "        return mu\n",
    "    \n",
    "    def generate_X_y(self, factor_data, returns_data, n_lookback, n_lookforward):\n",
    "        X, y = list(), list()\n",
    "        in_start = 0\n",
    "        for i in range(len(factor_data)):\n",
    "            in_end = in_start + n_lookback\n",
    "            out_end = in_end + n_lookforward\n",
    "            # ensure we have enough data for this instance\n",
    "            if out_end <= len(factor_data):\n",
    "                X.append(factor_data[in_start:in_end,:])\n",
    "                y.append(returns_data[in_end:out_end,:])\n",
    "            in_start += 1\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def traintest_split(self, X, y):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        return X_train, X_test, y_train, y_test   \n",
    "    \n",
    "    def build_model(self, train_x, train_y):\n",
    "        # define parameters\n",
    "        verbose, epochs, batch_size = 0, 50, 16\n",
    "        n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "\n",
    "        # define model\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(200, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "        model.add(RepeatVector(n_outputs))\n",
    "        model.add(LSTM(200, activation='relu', return_sequences=True))\n",
    "        model.add(TimeDistributed(Dense(100, activation='relu')))\n",
    "        model.add(TimeDistributed(Dense(train_y.shape[2])))\n",
    "        model.compile(loss='mse', optimizer='adam')\n",
    "        # fit network\n",
    "        model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "        return model\n",
    "    \n",
    "    def forecast(self, model, history, n_lookback):\n",
    "        # flatten data\n",
    "        data = np.array(history)\n",
    "        data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "        # retrieve last observations for lookback data\n",
    "        input_x = data[-n_lookback:, :]\n",
    "        # reshape into [1, n_lookback, n]\n",
    "        input_x = input_x.reshape((1, input_x.shape[0], input_x.shape[1]))\n",
    "        # forecast the next set\n",
    "        yhat = model.predict(input_x, verbose=0)\n",
    "        # we only want the vector forecast\n",
    "        yhat = yhat[0]\n",
    "        return yhat\n",
    "\n",
    "    def evaluate_forecasts(self, actual, predicted):\n",
    "        # calculate overall RMSE\n",
    "        s = 0\n",
    "        for row in range(actual.shape[0]):\n",
    "            for col in range(actual.shape[1]):\n",
    "                for k in range(actual.shape[2]):\n",
    "                    s += (actual[row, col, k] - predicted[row, col, k])**2\n",
    "        score = sqrt(s / (actual.shape[0] * actual.shape[1] * actual.shape[2]))\n",
    "        return score\n",
    "\n",
    "    def evaluate_model(self, train_x, train_y, test_x, test_y, n_lookback):\n",
    "        # fit model\n",
    "        model = self.build_model(train_x, train_y)\n",
    "        history = [x for x in train_x]\n",
    "        # walk-forward validation \n",
    "        predictions = list()\n",
    "        for i in range(len(test_x)):\n",
    "            yhat_sequence = self.forecast(model, history, n_lookback)\n",
    "            # store the predictions\n",
    "            predictions.append(yhat_sequence)\n",
    "            # get real observation and add to history for predicting the next set\n",
    "            history.append(test_x[i, :])\n",
    "        # evaluate predictions \n",
    "        predictions = np.array(predictions)\n",
    "        score = self.evaluate_forecasts(test_y, predictions)\n",
    "        plt.plot(model.history.history['loss'])\n",
    "        #plt.plot(model.history.history['val_loss'])\n",
    "        return score\n",
    "    \n",
    "    def get_prediction(self, train_x, train_y, factor_data, lookback):\n",
    "        model = self.build_model(train_x, train_y)\n",
    "        return self.forecast(model, factor_data.tail(lookback), lookback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
