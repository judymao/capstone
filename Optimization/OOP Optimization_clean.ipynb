{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "import hmmlearn as hmm\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import cvxpy as cp\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "## Additions below\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Main\n",
    "\n",
    "#Set up Data\n",
    "price_data = pd.read_csv(\"../Data/sp500df.csv\", index_col='Date')\n",
    "rfr = pd.DataFrame({'risk_free': [0.01]*len(price_data.index)}, index = price_data.index)\n",
    "data_set = Data(price_data, rfr)\n",
    "data_set.set_factor_returns()\n",
    "\n",
    "#Set Up Portfolio\n",
    "num_stocks=data_set.get_num_stocks()\n",
    "port= Portfolio(data_set)\n",
    "\n",
    "#Set Up model\n",
    "start_date = \"2014-10-31\"\n",
    "end_date = \"2017-11-01\"\n",
    "lookback = 20\n",
    "lookahead = 5\n",
    "lam = 0.9\n",
    "trans_coeff = 0.2\n",
    "holding_coeff = 0.2\n",
    "conf_level = 0.95\n",
    "\n",
    "# Define constraints to use\n",
    "constr_list = [\"no_short\", \"cardinality\", \"asset_limit_cardinality\"]\n",
    "constr_model = Constraints(constr_list)\n",
    "\n",
    "cost_model = Costs(trans_coeff, holding_coeff)\n",
    "cost_model.replicate_cost_coeff(num_stocks, lookahead)\n",
    "\n",
    "opt_model = Model(lam)\n",
    "risk_model = Risks(\"ellip\", conf_level)\n",
    "\n",
    "regress_weighting = [0,0.5,0.5,0]\n",
    "factor_model = FactorModel(lookahead, lookback, regress_weighting)\n",
    "\n",
    "back_test_ex = Backtest(start_date, end_date, lookback, lookahead)\n",
    "back_test_ex.run(data_set, port, factor_model, opt_model, constr_model, cost_model, risk_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "port.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "port.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    #Anything Data Related\n",
    "    def __init__(self, stock_prices, risk_free, universe=None):\n",
    "        #TO-DO: Add initialization of market cap\n",
    "        \n",
    "        if not universe:\n",
    "            universe = stock_prices.columns\n",
    "            \n",
    "        if type(universe[0])== int:\n",
    "            self.stock_prices=stock_prices.iloc[:,universe]\n",
    "\n",
    "        else:\n",
    "            self.stock_prices=stock_prices[universe]\n",
    "        \n",
    "        self.risk_free = risk_free\n",
    "        self.risk_free.index = pd.to_datetime(self.risk_free.index)\n",
    "        \n",
    "        self.stock_prices.index= pd.to_datetime(self.stock_prices.index)\n",
    "        self.factor_returns=None\n",
    "        self.stock_returns=self.get_stock_returns()\n",
    "        return\n",
    "    \n",
    "    def get_stock_returns(self, period='M'):\n",
    "        price = self.stock_prices.resample(period).last()\n",
    "\n",
    "        # Calculate the percent change\n",
    "        ret_data = price.pct_change()[1:]\n",
    "\n",
    "        # Convert from series to dataframe\n",
    "        ret_data = pd.DataFrame(ret_data)\n",
    "        \n",
    "        # Add in risk-free rate\n",
    "        ret_data = pd.concat([ret_data, self.risk_free.resample(period).last()], axis=1, join='inner')\n",
    "            \n",
    "        return ret_data\n",
    "    \n",
    "    def set_factor_returns(self, factor_type='FF', period='M'):\n",
    "        if factor_type == 'CAPM':\n",
    "            self.factor_returns = self.get_CAPM_returns(period)\n",
    "        \n",
    "        elif factor_type == 'FF':\n",
    "            self.factor_returns = self.get_FF_returns(period)\n",
    "            \n",
    "        elif factor_type == 'Carhart':\n",
    "            self.factor_returns = self.get_Carhart_returns(period)\n",
    "            \n",
    "        elif factor_type == 'PCA':\n",
    "            self.factor_returns = self.get_PCA_returns(period)\n",
    "        \n",
    "        else:\n",
    "            print(\"Invalid input: Please select one of the following factor types: CAPM, FF, Carhart or PCA.\")\n",
    "        \n",
    "        return   \n",
    "    \n",
    "    def get_FF_returns(self, period='M'):\n",
    "        ff_url = \"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_Factors_CSV.zip\"    \n",
    "        # Download the file and save it  \n",
    "        urllib.request.urlretrieve(ff_url,'fama_french.zip')\n",
    "        zip_file = zipfile.ZipFile('fama_french.zip', 'r')    \n",
    "        # Extact the file data\n",
    "        zip_file.extractall()\n",
    "        zip_file.close()    \n",
    "        ff_factors = pd.read_csv('F-F_Research_Data_Factors.csv', skiprows = 3, index_col = 0)   \n",
    "        # Skip null rows\n",
    "        ff_row = ff_factors.isnull().any(1).to_numpy().nonzero()[0][0]\n",
    "\n",
    "        # Read the csv file again with skipped rows\n",
    "        ff_factors = pd.read_csv('F-F_Research_Data_Factors.csv', skiprows = 3, nrows = ff_row, index_col = 0)\n",
    "\n",
    "        # Format the date index\n",
    "        ff_factors.index = pd.to_datetime(ff_factors.index, format= '%Y%m')\n",
    "\n",
    "        # Format dates to end of month\n",
    "        ff_factors.index = ff_factors.index + pd.offsets.MonthEnd()\n",
    "\n",
    "        # Resample the data to correct frequency\n",
    "        ff_factors = ff_factors.resample(period).last()\n",
    "\n",
    "        # Convert from percent to decimal\n",
    "        ff_factors = ff_factors.apply(lambda x: x/ 100)\n",
    "\n",
    "        return ff_factors\n",
    "    \n",
    "    def get_CAPM_returns(self, period='M'):\n",
    "        ff_factors = self.get_FF_returns(period)\n",
    "        \n",
    "        # Remove the unnecessary factors\n",
    "        capm_factors = ff_factors.iloc[:, 0]\n",
    "        \n",
    "        return capm_factors\n",
    "    \n",
    "    def get_Carhart_returns(self, period='M'):\n",
    "        ff_factors = self.get_FF_returns(period)\n",
    "\n",
    "        # Get the momentum factor\n",
    "        momentum_url = \"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Momentum_Factor_CSV.zip\"\n",
    "\n",
    "        # Download the file and save it  \n",
    "        urllib.request.urlretrieve(momentum_url,'momentum.zip')\n",
    "        zip_file = zipfile.ZipFile('momentum.zip', 'r')\n",
    "\n",
    "        # Extact the file data\n",
    "        zip_file.extractall()\n",
    "        zip_file.close()\n",
    "\n",
    "        momentum_factor = pd.read_csv('F-F_Momentum_Factor.csv', skiprows = 13, index_col = 0)\n",
    "\n",
    "        # Skip null rows\n",
    "        row = momentum_factor.isnull().any(1).to_numpy().nonzero()[0][0]\n",
    "\n",
    "        # Read the csv file again with skipped rows\n",
    "        momentum_factor = pd.read_csv('F-F_Momentum_Factor.csv', skiprows = 13, nrows = row, index_col = 0)\n",
    "\n",
    "        # Format the date index\n",
    "        momentum_factor.index = pd.to_datetime(momentum_factor.index, format= '%Y%m')\n",
    "\n",
    "        # Format dates to end of month\n",
    "        momentum_factor.index = momentum_factor.index + pd.offsets.MonthEnd()\n",
    "\n",
    "         # Resample the data to correct frequency\n",
    "        momentum_factor = momentum_factor.resample(period).last()\n",
    "\n",
    "        # Convert from percent to decimal\n",
    "        momentum_factor = momentum_factor.apply(lambda x: x/ 100)\n",
    "\n",
    "        # Combine to create the carhart_factors\n",
    "        carhart_factors = pd.concat([ff_factors, momentum_factor], axis=1).dropna()\n",
    "\n",
    "        return carhart_factors\n",
    "    \n",
    "    def get_PCA_returns(self, period='M'):\n",
    "        exRets = self.get_stock_returns(period=\"D\")\n",
    "        num_stocks = len(exRets.columns)\n",
    "        returns_mat = exRets.to_numpy()\n",
    "        n_dates = returns_mat.shape[0]\n",
    "        n_assets = returns_mat.shape[1]\n",
    "        \n",
    "        demeaned = (returns_mat - returns_mat.mean(axis=0)).transpose()\n",
    "        sigma = 1/(n_dates - 1)*np.matmul(demeaned,demeaned.transpose())\n",
    "        eigval, eigvec = np.linalg.eig(sigma)\n",
    "        \n",
    "        principal_components = np.matmul(eigvec.transpose(),demeaned).transpose()\n",
    "        pca_factors = np.real(principal_components[:,0:100])\n",
    "        \n",
    "        pca_df = pd.DataFrame(pca_factors, index = exRets.index, columns = [str(i) for i in range(num_stocks)])\n",
    "        pca_df = pca_df.resample(period).last()\n",
    "        \n",
    "        return pca_df\n",
    "    \n",
    "    def get_index_from_date(self, date_index_df, date):\n",
    "        return date_index_df.index.get_loc(date)\n",
    "    \n",
    "    def get_lookback_data(self, date_index_df, date, lookback):\n",
    "        end_idx= self.get_index_from_date(date_index_df, date)\n",
    "        return date_index_df.iloc[end_idx-lookback:end_idx]      \n",
    "    \n",
    "    def get_num_stocks(self):\n",
    "        return len(self.stock_returns.columns)\n",
    "\n",
    "    \n",
    "class Portfolio:\n",
    "    #Anything Portfolio related: weights, returns, date-stamped\n",
    "    def __init__(self, data):       \n",
    "        num_assets=len(data.stock_returns.columns)\n",
    "        self.weights= np.array([[0 for i in range(num_assets-1)] + [1]]) # 0 weight on stock, 1 in risk-free market\n",
    "        self.returns= np.array([])\n",
    "        self.dates= []\n",
    "        self.risk_free = data.get_stock_returns()['risk_free']\n",
    "        return\n",
    "        \n",
    "    def update_weights(self, new_weights):\n",
    "        \n",
    "        new_weights=np.array([new_weights])        \n",
    "        self.weights=np.append(self.weights,new_weights,axis=0)\n",
    "        return\n",
    "     \n",
    "    def update_returns(self, new_returns):\n",
    "        self.returns=np.append(self.returns, new_returns)\n",
    "        return\n",
    "\n",
    "    def update_dates(self, new_dates):\n",
    "       \n",
    "        self.dates.append(new_dates)\n",
    "        return\n",
    "        \n",
    "    def get_Sharpe(self):\n",
    "        recent_date = self.dates[-1]\n",
    "        sigma = np.std(self.returns - self.risk_free[self.dates])\n",
    "        sharpe_ratio = ((np.prod(1+self.returns)-1) - self.risk_free[recent_date])/sigma\n",
    "        return sharpe_ratio\n",
    "        \n",
    "    def plot(self):\n",
    "        port_cumu_returns = np.array([x+1 for x in self.returns]).cumprod()\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.plot(self.dates, port_cumu_returns)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Cumulative Return\")\n",
    "        plt.show()\n",
    "    \n",
    "\n",
    "class Constraints:\n",
    "        #List of all constraints\n",
    "        def __init__(self, constr_list=['cardinality','asset_limit_cardinality','no_short'], \n",
    "                     upper_limit=-1, lower_limit=1, stock_limit=20):\n",
    "            self.upper_limit = upper_limit\n",
    "            self.lower_limit = lower_limit\n",
    "            self.stock_limit = stock_limit\n",
    "            self.constr_list = constr_list\n",
    "            self.value=[]\n",
    "\n",
    "        \n",
    "        def set_constraints(self, weights, y):\n",
    "            # Set weight unity\n",
    "            self.value += [cp.sum(weights,axis=0)==1]\n",
    "            \n",
    "            if \"cardinality\" in self.constr_list:\n",
    "                self.value+= [cp.sum(y,axis=0)== self.stock_limit]   \n",
    "                \n",
    "            if \"no_short\" in self.constr_list:\n",
    "                self.value+=  [weights>=0]\n",
    "                \n",
    "            if \"asset_limit_cardinality\" in self.constr_list:\n",
    "                cardinality_upper_limit= cp.multiply(self.upper_limit, y)\n",
    "                cardinality_lower_limit = cp.multiply(self.lower_limit,y)\n",
    "                self.value+=  [weights>=cardinality_lower_limit, weights<=cardinality_upper_limit]\n",
    "\n",
    "            elif \"asset_limit\" in self.constr_list:\n",
    "                self.value+=[weights>=self.upper_limit, weights<=self.lower_limit]\n",
    "        \n",
    "            return\n",
    "\n",
    "\n",
    "class Risks:\n",
    "    def __init__(self, risk_type=\"MVO\", conf_lvl=0):\n",
    "        self.value=0\n",
    "        self.risk_type=risk_type\n",
    "        self.conf_lvl=conf_lvl\n",
    "        return\n",
    "        \n",
    "    def set_risk(self, weights, Q, lookahead):\n",
    "        portfolio_risk=0\n",
    "        robustness_cost=0\n",
    "        num_stocks = weights.shape[1]\n",
    "        \n",
    "        for i in range(lookahead):\n",
    "            portfolio_risk += cp.quad_form(weights[:,i],Q[i])        \n",
    "        self.value = portfolio_risk\n",
    "        \n",
    "        if self.risk_type == \"rect\":\n",
    "            for i in range(lookahead):\n",
    "                delta = stats.norm.ppf(self.conf_lvl)*np.sqrt(np.diag(Q[i]/num_stocks))\n",
    "                robustness_cost += delta@cp.abs(weights[:,i])\n",
    "            self.value += robustness_cost\n",
    "        \n",
    "        elif self.risk_type == \"ellip\":\n",
    "            for i in range(lookahead):\n",
    "         \n",
    "                penalty = cp.norm(np.sqrt(np.diag(Q[i]/num_stocks))@weights[:,i],2)\n",
    "          \n",
    "                robustness_cost += stats.chi2.ppf(self.conf_lvl, num_stocks)*penalty\n",
    "            self.value += robustness_cost\n",
    "            \n",
    "        elif self.risk_type == \"cvar\":\n",
    "            pass\n",
    "        \n",
    "        elif self.risk_type == 'B-L':\n",
    "            self.value = 0\n",
    "            pass\n",
    "        \n",
    "        return\n",
    "    \n",
    "\n",
    "class Costs:\n",
    "    def __init__(self, trans_coeff, holding_coeff):\n",
    "        self.holding_cost = 0\n",
    "        self.trans_cost = 0\n",
    "        self.trans_coeff = trans_coeff\n",
    "        self.holding_coeff = holding_coeff\n",
    "        return\n",
    "        \n",
    "    def replicate_cost_coeff(self, num_stocks, lookahead):\n",
    "        trans_cost_repl = np.ones((num_stocks,lookahead))/100\n",
    "        holding_cost_repl = np.ones((num_stocks, lookahead))/100\n",
    "        self.trans_coeff = trans_cost_repl*self.trans_coeff\n",
    "        self.holding_coeff = holding_cost_repl*self.holding_coeff\n",
    "        return\n",
    "    \n",
    "    def set_holding_cost(self, weights_new):\n",
    "        self.holding_cost += cp.sum(cp.multiply(self.holding_coeff, cp.neg(weights_new)))\n",
    "        return\n",
    "        \n",
    "    def calc_trans_cost(self, weights_new, weights_old, trans_coeff):\n",
    "        abs_trade= cp.abs(weights_new-weights_old)\n",
    "        return cp.sum(cp.multiply(trans_coeff, abs_trade))       \n",
    "    \n",
    "    \n",
    "    def set_trans_cost(self, weights_new,weights_old):\n",
    "        \n",
    "        weights_curr= weights_new[:,0]\n",
    "        if weights_new.shape[1]>1:         \n",
    "            weights_future = weights_new[:,1:]\n",
    "            weights_future_shift = weights_new[:,:-1]\n",
    "            self.trans_cost = self.calc_trans_cost(weights_future, weights_future_shift, self.trans_coeff[:,1:])\n",
    "\n",
    "        self.trans_cost += self.calc_trans_cost(weights_curr, weights_old,self.trans_coeff[:,0])\n",
    "        return\n",
    "\n",
    "    \n",
    "    \n",
    "class Model:\n",
    "    def __init__(self, lam):\n",
    "        self.opt_weights = 0\n",
    "        self.status = None\n",
    "        self.lam = lam\n",
    "\n",
    "        return\n",
    "        \n",
    "    def MVO(self, port, mu , Q, look_ahead, constr_model, cost_model, risk_model):\n",
    "        \n",
    "        mu_np = np.array(mu)\n",
    "        Q_np = np.array(Q)\n",
    "        \n",
    "        num_stocks = port.weights.shape[1]\n",
    "        \n",
    "        #Construct optimization problem\n",
    "        weights = cp.Variable((num_stocks,look_ahead))\n",
    "        y = cp.Variable((num_stocks,look_ahead), integer=True)\n",
    "        \n",
    "        weights_prev= port.weights[-1,:]   \n",
    "        \n",
    "        # Set model parameters\n",
    "        cost_model.set_trans_cost(weights, weights_prev)\n",
    "        cost_model.set_holding_cost(weights)    \n",
    "        constr_model.set_constraints(weights, y)\n",
    "        risk_model.set_risk(weights, Q,look_ahead)\n",
    "\n",
    "        # Get portfolio return\n",
    "        portfolio_return_per_period = mu_np@weights\n",
    "        portfolio_return = cp.trace(portfolio_return_per_period)\n",
    "\n",
    "        objective= cp.Maximize(portfolio_return - self.lam*risk_model.value - cost_model.holding_cost - cost_model.trans_cost)\n",
    "        \n",
    "        #Construct Problem and Solve\n",
    "        prob= cp.Problem(objective, constr_model.value)\n",
    "        result=prob.solve(solver=\"GUROBI\", verbose=False)\n",
    "        self.status= prob.status\n",
    "        if self.status == \"optimal\":\n",
    "            self.opt_weights=np.array(weights.value)[:,1]\n",
    "        else:\n",
    "            self.opt_weights=weights_prev.T\n",
    "     \n",
    "        return self.opt_weights\n",
    "        \n",
    "    def BL(self):\n",
    "        return\n",
    "    \n",
    "    \n",
    "class Backtest:\n",
    "    def __init__(self, start_date, end_date, lookback, lookahead):\n",
    "        self.rebal_freq = 'M'\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.lookback = lookback\n",
    "        self.lookahead = lookahead\n",
    "        self.reb_dates = None\n",
    "        return\n",
    "        \n",
    "    \n",
    "    def run(self, data, portfolio, factor_model, optimizer, constr_model, cost_model, risk_model):      \n",
    "        stock_return= data.stock_returns      \n",
    "        self.reb_dates= np.array(data.stock_returns.loc[self.start_date:self.end_date].index)    \n",
    " \n",
    "\n",
    "        for t in self.reb_dates:\n",
    "            mu, Q = factor_model.get_param_estimate(t, data)\n",
    "            \n",
    "            if risk_model.risk_type in ('MVO', 'ellip', 'rect', 'cvar'):\n",
    "                weights = optimizer.MVO(portfolio, mu , Q,self.lookahead, constr_model, cost_model, risk_model)     \n",
    "            \n",
    "            elif risk_model.risk_type == 'B-L':\n",
    "                weights = optimizer.BL()\n",
    "            \n",
    "            portfolio.update_dates(t)\n",
    "            portfolio.update_weights(weights)\n",
    "            portfolio.update_returns(np.dot(weights,stock_return.loc[t]))\n",
    "\n",
    "            ##How the lambdas influence\n",
    "            ##When to use MVO.. when to use CVaR... implement a CVaR\n",
    "\n",
    "        return portfolio.get_Sharpe()\n",
    "\n",
    "\n",
    "    def grid_search(self, data, portfolio, model, trans_coeff=0.2, hold_coeff=0.2, lam=0.9, conf_level=0.95):\n",
    "\n",
    "#         # Overall - currently test values are used\n",
    "#         pot_lookaheads = [1, 3, 6, 12, 60]\n",
    "#         pot_lookbacks = [2, 3, 6, 12, 60]\n",
    "\n",
    "#         # Factor Models\n",
    "#         factor_models = ['CAPM', 'FF', 'Carhart', 'PCA'] # Data\n",
    "#         regressions = ['linear', 'lasso', 'ridge', 'SVR'] # FactorModel\n",
    "\n",
    "#         # Constraints\n",
    "#         cardinalities = ['', 'cardinality']\n",
    "#         asset_limits = ['asset_limit_cardinality', 'asset_limit']\n",
    "#         no_shorts = ['', 'no_short']\n",
    "#         constraints_list = [cardinalities, asset_limits, no_shorts]\n",
    "\n",
    "#         stock_limits = list(range(5, 501, 5))\n",
    "\n",
    "#         # Optimization\n",
    "#         MVO_robustness = ['', 'rectangular', 'elliptical']\n",
    "\n",
    "        # Overall\n",
    "        pot_lookaheads = [5]\n",
    "        pot_lookbacks = [20]\n",
    "\n",
    "        # Factor Models\n",
    "        factor_models = ['FF', 'PCA']  # Data\n",
    "        weights = [[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1],[0.25,0.25,0.25,0.25],\n",
    "                       [0,0.5,0.5,0],[0,0.25,0.25,0.5],[0,0,0.5,0.5]]\n",
    "\n",
    "        # Constraints\n",
    "        cardinalities = ['cardinality']\n",
    "        asset_limits = ['asset_limit_cardinality', 'asset_limit']\n",
    "        no_shorts = ['no_short']\n",
    "        constraints_list = [cardinalities, asset_limits, no_shorts]\n",
    "\n",
    "        stock_limits = list(range(5, 21, 5))\n",
    "        upper_asset_limits = [1]\n",
    "        lower_asset_limits = [-1]\n",
    "\n",
    "        # Optimization\n",
    "        MVO_robustness = ['ellip']\n",
    "\n",
    "        # list of sharpe ratios per parameter combination\n",
    "        sharpe_ratios = []\n",
    "\n",
    "        # list of parameter combinations corresponding to sharpe ratio\n",
    "        parameter_combos = []\n",
    "\n",
    "        for combo in tqdm(list(itertools.product(factor_models, weights, \\\n",
    "                                                 list(itertools.product(*constraints_list)), stock_limits, \\\n",
    "                                                 upper_asset_limits, lower_asset_limits, MVO_robustness))):\n",
    "\n",
    "            # Store the combination\n",
    "            curr_combo = {'rebalance_freq': 'M', 'factor_model': combo[0], 'weights': combo[1],\n",
    "                          'constraints_list': list(combo[2]), 'stock_limit': combo[3], 'upper_asset_limit': combo[4],\n",
    "                          'lower_asset_limit': combo[5], 'robustness': combo[6]}\n",
    "\n",
    "            # Initial Setup\n",
    "            data.set_factor_returns(curr_combo['factor_model'], curr_combo['rebalance_freq'])\n",
    "\n",
    "            num_stocks=data_set.get_num_stocks()\n",
    "            cost_model = Costs(trans_coeff, hold_coeff)\n",
    "\n",
    "            # Get lookaheads that are multiples of the rebalancing frequency and <= 60 months\n",
    "            if curr_combo['rebalance_freq'] == 'M':\n",
    "                first = 1\n",
    "            else:\n",
    "                first = int(curr_combo['rebalance_freq'][0])\n",
    "\n",
    "            lookaheads = list(itertools.compress(pot_lookaheads, [look >= first for look in pot_lookaheads]))\n",
    "            lookbacks = list(itertools.compress(pot_lookbacks, [look >= first for look in pot_lookbacks]))\n",
    "\n",
    "            for lookahead in lookaheads:\n",
    "                curr_combo['lookahead'] = lookahead\n",
    "                for lookback in lookbacks:\n",
    "                    curr_combo['lookback'] = lookback\n",
    "\n",
    "                    # Continue Setup\n",
    "                    cost_model.replicate_cost_coeff(num_stocks, lookahead)\n",
    "                    constr_model = Constraints(curr_combo['constraints_list'])\n",
    "\n",
    "                    risk_model = Risks(curr_combo['robustness'], conf_level)\n",
    "\n",
    "                    # Run backtest\n",
    "                    factor = FactorModel(curr_combo['lookahead'], curr_combo['lookback'],\n",
    "                                         curr_combo['weights'])\n",
    "                    sharpe = self.run(data_set, portfolio, factor, model, constr_model, cost_model, risk_model)\n",
    "\n",
    "                    # Update results\n",
    "                    sharpe_ratios.append(sharpe)\n",
    "                    parameter_combos.append(curr_combo)\n",
    "\n",
    "        return sharpe_ratios, parameter_combos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorModel:\n",
    "    def __init__(self, lookahead, lookback, regress_weighting):\n",
    "        \n",
    "        \"\"\"\n",
    "        lookahead: number of periods in the future to estimate\n",
    "        lookback: number of periods in the past to use for estimations\n",
    "        regress_weighting: array of size 4 with weight corresponding to each regression type; adds up to 1; \n",
    "        order is linear, lasso, ridge, SVR; in the case where there is one 1 and the rest 0's, there is no ensembling;\n",
    "        can artifically call LSTM by setting all weights to 0\n",
    "        \"\"\"\n",
    "        self.lookahead = lookahead\n",
    "        self.lookback = lookback\n",
    "        self.regress_weighting = regress_weighting\n",
    "        return\n",
    "               \n",
    "    def get_param_estimate(self, rebal_date, data):\n",
    "               \n",
    "        if sum(self.regress_weighting) == 0:\n",
    "            return self.get_mu_LSTM(rebal_date, data)\n",
    "\n",
    "        elif sum(self.regress_weighting) == 1:\n",
    "            return self.get_mu_Q_regression(rebal_date, data)\n",
    "        \n",
    "        else:\n",
    "            return \"ERROR: This regression weighting is not valid. Please make sure the weights sum to 1. You can also give all zeros for LSTM.\"\n",
    "        \n",
    "    def get_mu_Q_regression(self, rebal_date, data): \n",
    "        returns_data = data.stock_returns\n",
    "        factor_data = data.factor_returns\n",
    "        lookahead = self.lookahead\n",
    "        lookback = self.lookback\n",
    "        regress_weighting = self.regress_weighting\n",
    "                \n",
    "        # For keeping track of mu's and Q's from each period\n",
    "        mu_arr = []\n",
    "        Q_arr = []\n",
    "\n",
    "        n_factors = len(factor_data.columns)\n",
    "\n",
    "        returns_data = data.get_lookback_data(returns_data, rebal_date, lookback)\n",
    "        factor_data = data.get_lookback_data(factor_data, rebal_date, lookback)\n",
    "        \n",
    "        for i in range(0, lookahead):\n",
    "\n",
    "            # Calculate the factor covariance matrix\n",
    "            F = factor_data.loc[:, factor_data.columns != 'Ones'].cov()\n",
    "\n",
    "            # Calculate the factor expected excess return from historical data using the geometric mean\n",
    "            factor_data['Ones'] = [1 for i in range(len(factor_data))]\n",
    "            gmean = stats.gmean(factor_data + 1,axis=0) - 1\n",
    "\n",
    "            # Set up X and Y to determine alpha and beta\n",
    "            X = factor_data\n",
    "            Y = returns_data\n",
    "            X = X.to_numpy()\n",
    "            Y = Y.to_numpy()\n",
    "\n",
    "            \n",
    "            ### LINEAR REGRESSION\n",
    "        \n",
    "            model = LinearRegression().fit(X,Y)\n",
    "            alpha = model.intercept_\n",
    "            beta = model.coef_[:,0:n_factors]\n",
    "\n",
    "            # Calculate the residuals \n",
    "            alpha = np.reshape(alpha,(alpha.size,1))\n",
    "            epsilon = returns_data.to_numpy() - np.matmul(X, np.transpose(np.hstack((beta, alpha))))\n",
    "\n",
    "            # Calculate the residual variance with \"N - p - 1\" degrees of freedom\n",
    "            sigmaEp = np.sum(epsilon**2, axis=0) / (len(returns_data) - n_factors - 1)\n",
    "\n",
    "            #  Calculate the asset expected excess returns\n",
    "            mu_linear = model.predict([gmean])[0]\n",
    "\n",
    "            # Calculate the diagonal matrix of residuals and the asset covariance matrix\n",
    "            D = np.diag(sigmaEp)\n",
    "\n",
    "            # Calculate the covariance matrix\n",
    "            Q_linear = np.matmul(np.matmul(beta,F.to_numpy()),beta.T)+D\n",
    "\n",
    "\n",
    "            ### LASSO REGRESSION\n",
    "\n",
    "            model = Lasso().fit(X,Y)\n",
    "            alpha = model.intercept_\n",
    "            beta = model.coef_[:,0:n_factors]\n",
    "\n",
    "            # Calculate the residuals \n",
    "            alpha = np.reshape(alpha,(alpha.size,1))\n",
    "            epsilon = returns_data.to_numpy() - np.matmul(X, np.transpose(np.hstack((beta, alpha))))\n",
    "\n",
    "            # Calculate the residual variance with \"N - p - 1\" degrees of freedom\n",
    "            sigmaEp = np.sum(epsilon**2, axis=0) / (len(returns_data) - n_factors - 1)\n",
    "\n",
    "            #  Calculate the asset expected excess returns\n",
    "            mu_lasso = model.predict([gmean])[0]\n",
    "\n",
    "            # Calculate the diagonal matrix of residuals and the asset covariance matrix\n",
    "            D = np.diag(sigmaEp)\n",
    "\n",
    "            # Calculate the covariance matrix\n",
    "            Q_lasso = np.matmul(np.matmul(beta,F.to_numpy()),beta.T)+D\n",
    "\n",
    "\n",
    "            ### RIDGE REGRESSION\n",
    "\n",
    "            model = Ridge().fit(X,Y)\n",
    "            alpha = model.intercept_\n",
    "            beta = model.coef_[:,0:n_factors]\n",
    "\n",
    "            # Calculate the residuals \n",
    "            alpha = np.reshape(alpha,(alpha.size,1))\n",
    "            epsilon = returns_data.to_numpy() - np.matmul(X, np.transpose(np.hstack((beta, alpha))))\n",
    "\n",
    "            # Calculate the residual variance with \"N - p - 1\" degrees of freedom\n",
    "            sigmaEp = np.sum(epsilon**2, axis=0) / (len(returns_data) - n_factors - 1)\n",
    "\n",
    "            #  Calculate the asset expected excess returns\n",
    "            mu_ridge = model.predict([gmean])[0]\n",
    "\n",
    "            # Calculate the diagonal matrix of residuals and the asset covariance matrix\n",
    "            D = np.diag(sigmaEp)\n",
    "\n",
    "            # Calculate the covariance matrix\n",
    "            Q_ridge = np.matmul(np.matmul(beta,F.to_numpy()),beta.T)+D\n",
    "\n",
    "\n",
    "            ### SUPPORT VECTOR REGRESSION\n",
    "\n",
    "            model = make_pipeline(StandardScaler(), MultiOutputRegressor(LinearSVR(C=1, dual=False, loss=\"squared_epsilon_insensitive\"))).fit(X, Y)\n",
    "            beta = np.array([[model.named_steps['multioutputregressor'].estimators_[i].coef_[0:n_factors] for i in range(len(model.named_steps['multioutputregressor'].estimators_))]])[0]\n",
    "            alpha = np.array([model.named_steps['multioutputregressor'].estimators_[i].intercept_[0] for i in range(len(model.named_steps['multioutputregressor'].estimators_))])\n",
    "\n",
    "            # Calculate the residuals \n",
    "            alpha = np.reshape(alpha,(alpha.size,1))\n",
    "            epsilon = returns_data.to_numpy() - np.matmul(X, np.transpose(np.hstack((beta, alpha))))\n",
    "\n",
    "            # Calculate the residual variance with \"N - p - 1\" degrees of freedom\n",
    "            sigmaEp = np.sum(epsilon**2, axis=0) / (len(returns_data) - n_factors - 1)\n",
    "\n",
    "            #  Calculate the asset expected excess returns\n",
    "            mu_SVR = model.predict([gmean])[0]\n",
    "\n",
    "            # Calculate the diagonal matrix of residuals and the asset covariance matrix\n",
    "            D = np.diag(sigmaEp)\n",
    "\n",
    "            # Calculate the covariance matrix\n",
    "            Q_SVR = np.matmul(np.matmul(beta,F.to_numpy()),beta.T)+D\n",
    "\n",
    "        \n",
    "            # Ensemble the methods\n",
    "            mu = regress_weighting[0]*mu_linear + regress_weighting[1]*mu_lasso + regress_weighting[2]*mu_ridge + regress_weighting[3]*mu_SVR\n",
    "            Q = regress_weighting[0]*Q_linear + regress_weighting[1]*Q_lasso + regress_weighting[2]*Q_ridge + regress_weighting[3]*Q_SVR\n",
    "\n",
    "            # Add mu and Q to array\n",
    "            mu_arr.append(mu)\n",
    "            Q_arr.append(Q)\n",
    "\n",
    "            # Update for next time step\n",
    "            factor_data = factor_data[1:]\n",
    "            factor_append = pd.Series(gmean, index = factor_data.columns)\n",
    "            factor_data = factor_data.append(factor_append, ignore_index=True)\n",
    "\n",
    "            returns_data = returns_data[1:]\n",
    "            mu_append = pd.Series(mu, index=returns_data.columns)\n",
    "            returns_data = returns_data.append(mu_append, ignore_index=True)   \n",
    "\n",
    "        return mu_arr, Q_arr\n",
    "        \n",
    "    def get_mu_LSTM(self, rebal_date, data): \n",
    "        returns_data = data.stock_returns\n",
    "        factor_data = data.factor_returns\n",
    "        \n",
    "        lookahead = self.lookahead\n",
    "        lookback = self.lookback\n",
    "        regress_weighting = self.regress_weighting\n",
    "\n",
    "        returns_data = data.get_lookback_data(returns_data, rebal_date, lookback)\n",
    "        factor_data = data.get_lookback_data(factor_data, rebal_date, lookback)\n",
    "        \n",
    "        tempx, tempy = self.generate_X_y(factor_data.values, returns_data.values, lookback, lookahead)\n",
    "        train_x, test_x, train_y, test_y = self.traintest_split(tempx, tempy)\n",
    "\n",
    "        # scale inputs\n",
    "        scaled_train_x = (train_x - train_x.min())/(train_x.max() - train_x.min())\n",
    "        scaled_test_x = (test_x - test_x.min())/(test_x.max() - test_x.min())\n",
    "        scaled_train_y = (train_y - train_y.min())/(train_y.max() - train_y.min())\n",
    "        scaled_test_y = (test_y - test_y.min())/(test_y.max() - test_y.min())\n",
    "\n",
    "        mu = self.get_prediction(train_x, train_y, factor_data, lookback)\n",
    "        return mu\n",
    "    \n",
    "    def generate_X_y(self, factor_data, returns_data, n_lookback, n_lookforward):\n",
    "        X, y = list(), list()\n",
    "        in_start = 0\n",
    "        for i in range(len(factor_data)):\n",
    "            in_end = in_start + n_lookback\n",
    "            out_end = in_end + n_lookforward\n",
    "            # ensure we have enough data for this instance\n",
    "            if out_end <= len(factor_data):\n",
    "                X.append(factor_data[in_start:in_end,:])\n",
    "                y.append(returns_data[in_end:out_end,:])\n",
    "            in_start += 1\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def traintest_split(self, X, y):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        return X_train, X_test, y_train, y_test   \n",
    "    \n",
    "    def build_model(self, train_x, train_y):\n",
    "        # define parameters\n",
    "        verbose, epochs, batch_size = 0, 50, 16\n",
    "        n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "\n",
    "        # define model\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(200, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "        model.add(RepeatVector(n_outputs))\n",
    "        model.add(LSTM(200, activation='relu', return_sequences=True))\n",
    "        model.add(TimeDistributed(Dense(100, activation='relu')))\n",
    "        model.add(TimeDistributed(Dense(train_y.shape[2])))\n",
    "        model.compile(loss='mse', optimizer='adam')\n",
    "        # fit network\n",
    "        model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "        return model\n",
    "    \n",
    "    def forecast(self, model, history, n_lookback):\n",
    "        # flatten data\n",
    "        data = np.array(history)\n",
    "        data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "        # retrieve last observations for lookback data\n",
    "        input_x = data[-n_lookback:, :]\n",
    "        # reshape into [1, n_lookback, n]\n",
    "        input_x = input_x.reshape((1, input_x.shape[0], input_x.shape[1]))\n",
    "        # forecast the next set\n",
    "        yhat = model.predict(input_x, verbose=0)\n",
    "        # we only want the vector forecast\n",
    "        yhat = yhat[0]\n",
    "        return yhat\n",
    "\n",
    "    def evaluate_forecasts(self, actual, predicted):\n",
    "        # calculate overall RMSE\n",
    "        s = 0\n",
    "        for row in range(actual.shape[0]):\n",
    "            for col in range(actual.shape[1]):\n",
    "                for k in range(actual.shape[2]):\n",
    "                    s += (actual[row, col, k] - predicted[row, col, k])**2\n",
    "        score = sqrt(s / (actual.shape[0] * actual.shape[1] * actual.shape[2]))\n",
    "        return score\n",
    "\n",
    "    def evaluate_model(self, train_x, train_y, test_x, test_y, n_lookback):\n",
    "        # fit model\n",
    "        model = self.build_model(train_x, train_y)\n",
    "        history = [x for x in train_x]\n",
    "        # walk-forward validation \n",
    "        predictions = list()\n",
    "        for i in range(len(test_x)):\n",
    "            yhat_sequence = self.forecast(model, history, n_lookback)\n",
    "            # store the predictions\n",
    "            predictions.append(yhat_sequence)\n",
    "            # get real observation and add to history for predicting the next set\n",
    "            history.append(test_x[i, :])\n",
    "        # evaluate predictions \n",
    "        predictions = np.array(predictions)\n",
    "        score = self.evaluate_forecasts(test_y, predictions)\n",
    "        plt.plot(model.history.history['loss'])\n",
    "        #plt.plot(model.history.history['val_loss'])\n",
    "        return score\n",
    "    \n",
    "    def get_prediction(self, train_x, train_y, factor_data, lookback):\n",
    "        model = self.build_model(train_x, train_y)\n",
    "        return self.forecast(model, factor_data.tail(lookback), lookback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "278afcc71ccd44e3949c74c745926973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=128.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using license file /Users/judymao/gurobi.lic\n",
      "Academic license - for non-commercial use only - expires 2021-01-20\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-13253b18cdc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Define constraints to use\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBacktest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookahead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-3d7d812a3c8d>\u001b[0m in \u001b[0;36mgrid_search\u001b[0;34m(self, data, portfolio, model, trans_coeff, hold_coeff, lam, conf_level)\u001b[0m\n\u001b[1;32m    480\u001b[0m                     factor = FactorModel(curr_combo['lookahead'], curr_combo['lookback'],\n\u001b[1;32m    481\u001b[0m                                          curr_combo['weights'])\n\u001b[0;32m--> 482\u001b[0;31m                     \u001b[0msharpe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mportfolio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstr_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrisk_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m                     \u001b[0;31m# Update results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-3d7d812a3c8d>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, portfolio, factor_model, optimizer, constr_model, cost_model, risk_model)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreb_dates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m             \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactor_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_param_estimate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrisk_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrisk_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'MVO'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ellip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rect'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cvar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-282098184639>\u001b[0m in \u001b[0;36mget_param_estimate\u001b[0;34m(self, rebal_date, data)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregress_weighting\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mu_Q_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrebal_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-282098184639>\u001b[0m in \u001b[0;36mget_mu_Q_regression\u001b[0;34m(self, rebal_date, data)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;31m### SUPPORT VECTOR REGRESSION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiOutputRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLinearSVR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdual\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"squared_epsilon_insensitive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'multioutputregressor'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_factors\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'multioutputregressor'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'multioutputregressor'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'multioutputregressor'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/sklearn/multioutput.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 **fit_params_validated)\n\u001b[0;32m--> 176\u001b[0;31m             for i in range(y.shape[1]))\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1049\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    864\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 866\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/sklearn/multioutput.py\u001b[0m in \u001b[0;36m_fit_estimator\u001b[0;34m(estimator, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_fit_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mnew_object_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew_object_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mnew_object_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0mnew_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mnew_object_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mparams_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mestimator_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# XXX: not handling dictionaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mestimator_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrozenset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mestimator_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'get_params'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Set up Data\n",
    "price_data = pd.read_csv(\"../Data/sp500df.csv\", index_col='Date')\n",
    "rfr = pd.DataFrame({'risk_free': [0.01]*len(price_data.index)}, index = price_data.index)\n",
    "data_set = Data(price_data, rfr)\n",
    "data_set.set_factor_returns()\n",
    "\n",
    "#Set Up Portfolio and Model\n",
    "port=Portfolio(data_set)\n",
    "\n",
    "#Set Up model\n",
    "start_date= \"2014-10-31\"\n",
    "end_date= \"2017-11-01\"\n",
    "lookback=20\n",
    "lookahead=5\n",
    "lam=0.9\n",
    "model=Model(lam)\n",
    "\n",
    "# Define constraints to use\n",
    "test = Backtest(start_date, end_date, lookback, lookahead)\n",
    "test.grid_search(data_set, port, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=data_set.stock_returns.index[0]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    class Regime:\n",
    "        def __init__(self, data ,t):\n",
    "            train_data=None\n",
    "            train_dates=None\n",
    "            self.get_train_data(data,t)\n",
    "            \n",
    "        def get_train_data(self, data, t):\n",
    "            mkt_data = data.factor_returns[\"Mkt-RF\"]+data_set.factor_returns[\"RF\"]\n",
    "            first_date=\"2019-01-01\"\n",
    "            mkt_returns=mkt_data[first_date:t]\n",
    "            self.train_dates=mkt_returns.index\n",
    "            \n",
    "            mkt_returns=np.array(mkt_returns.values)\n",
    "            mkt_prices = 100*(np.array([x+1 for x in mkt_returns]).cumprod())\n",
    "            mkt_prices=np.expand_dims(mkt_prices,axis=1)\n",
    "            self.train_data= mkt_prices\n",
    "            \n",
    "            \n",
    "        def HMM (self, num_hs):\n",
    "            model=hmm.GaussianHMM(n_components=num_hs)\n",
    "            model.fit(self.train_data)\n",
    "            return model\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=\"2019-12-31\"\n",
    "reg=Regime(data_set,t)\n",
    "reg_model =reg.HMM(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model.means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=reg_model.predict(reg.train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=np.array(list(map(bool,out)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model.means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.scatter(reg.train_dates[test],reg.train_data[test])\n",
    "        plt.scatter(reg.train_dates[~test],reg.train_data[~test])\n",
    "\n",
    "        plt.xticks(rotation=45)\n",
    "        #plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Cumulative Return\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(reg.train_dates[~test],reg.train_data[~test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.train_dates[test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
